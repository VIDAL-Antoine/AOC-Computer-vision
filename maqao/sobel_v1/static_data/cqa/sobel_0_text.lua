_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = " - VEXTRACTF128: 12 occurrences\n - VINSERTF128: 4 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 16 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision. In C/C++, FP constants are double precision by default and must be suffixed by 'f' to make them single precision.",
          details = " - VCVTDQ2PS (INT32 to FP32, SIMD): 12 occurrences\n - VCVTPD2PS (FP64 to FP32, SIMD): 8 occurrences\n - VCVTPS2PD (FP32 to FP64, SIMD): 24 occurrences\n - VCVTTPS2DQ (FP32 to INT32, SIMD): 4 occurrences\n - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "24 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 160 FP arithmetical operations:\n - 64: addition or subtraction (all inside FMA instructions)\n - 96: multiply (64 inside FMA instructions)\nThe binary loop is loading 1120 bytes (140 double precision FP elements).\nThe binary loop is storing 96 bytes (12 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.13 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 192\nloop length        : 1098\nused x86 registers : 2\nused mmx registers : 0\nused xmm registers : 14\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 0\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 69.00 cycles\ninstruction queue    : 96.00 cycles\ndecoding             : 96.00 cycles\nmicro-operation queue: 96.00 cycles\nfront end            : 96.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n------------------------------------------------------------\nuops   | 417.00 | 43.00 | 19.00 | 19.00 | 1.50 | 1.50 | 3.00\ncycles | 417.00 | 43.00 | 19.00 | 19.00 | 1.50 | 1.50 | 3.00\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 0.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 96.00\nDispatch  : 417.00\nData deps.: 0.00\nOverall L1: 417.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 100%\nload    : 100%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\nINT+FP\nall     : 100%\nload    : 100%\nstore   : 100%\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 40%\nload   : 50%\nstore  : 50%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 39%\nFP\nall     : 37%\nload    : 50%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 32%\nINT+FP\nall     : 39%\nload    : 50%\nstore   : 50%\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 37%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 417.00 cycles. At this rate:\n - 2% of peak load performance is reached (2.69 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 0% of peak store performance is reached (0.23 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1270\n\nInstruction                           | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n--------------------------------------------------------------------------------------------------------------------------\nVMOVDQA (%RAX),%YMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x60(%RAX),%RAX                   | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVDQA -0x40(%RAX),%YMM0             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA -0x20(%RAX),%YMM7             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSHUFB 0x2e55(%RIP),%YMM1,%YMM8      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2e4c(%RIP),%YMM0,%YMM14     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM8,%YMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2e3d(%RIP),%YMM7,%YMM5      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM14,%YMM15            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2e4e(%RIP),%YMM1,%YMM10     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM5,%YMM2              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2e3f(%RIP),%YMM0,%YMM3      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM9,%YMM10,%YMM11              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2e31(%RIP),%YMM7,%YMM4      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM15,%YMM3,%YMM6               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM2,%YMM4,%YMM7                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERM2I128 $0x21,%YMM6,%YMM11,%YMM1   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM11,%YMM7,%YMM10  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM7,%YMM6,%YMM8    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x6,%YMM1,%YMM6,%YMM0       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM10,%YMM11,%YMM11    | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERM2I128 $0x21,%YMM11,%YMM0,%YMM3   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM3,%YMM11,%YMM6      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM8,%YMM7,%YMM9       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $0x4e,%YMM6,%YMM7              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPERM2I128 $0x21,%YMM0,%YMM9,%YMM14   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM7,%YMM6,%YMM1       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x5,%YMM14,%YMM0,%YMM15     | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERM2I128 $0x21,%YMM9,%YMM11,%YMM5   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERMQ $0x4e,%YMM15,%YMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPALIGNR $0x5,%YMM5,%YMM9,%YMM4       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVEXTRACTI128 $0x1,%YMM1,%XMM0         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM1,%YMM8                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM0,%YMM10                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM4,%YMM7                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPALIGNR $0xb,%YMM15,%YMM2,%YMM11     | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVEXTRACTI128 $0x1,%YMM10,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM10,%YMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM4,%XMM4         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM5,%YMM2                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVZXBW %XMM4,%YMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM8,%YMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM8,%XMM14        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM7,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM3,%YMM8                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM14,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM2,%YMM0                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM1,%YMM3                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM7,%XMM14        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM1,%XMM2         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM11,%YMM1                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM15,%YMM6                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM14,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM1,%YMM14                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM2,%YMM4                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM14,%YMM14               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM11       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM15,%YMM5                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVZXBW %XMM11,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM3,%YMM7                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM15,%YMM11               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM1,%XMM3         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM4,%YMM2                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM3,%YMM4                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM4,%YMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM15,%YMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM3,%YMM4                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM14,%YMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PS %YMM9,%YMM9                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVEXTRACTF128 $0x1,%YMM14,%XMM14       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM11,%YMM11               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMULPD %YMM13,%YMM3,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM9,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM9,%XMM9         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM10,%YMM10               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFMADD231PD %YMM12,%YMM15,%YMM3       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM9,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM14,%YMM9                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM9,%YMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD %YMM12,%YMM14,%YMM15      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM1,%YMM14                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM1,%XMM1         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVAPD %YMM15,%YMM9                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM6,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM6,%XMM6         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMULPD %YMM13,%YMM14,%YMM14           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD %YMM12,%YMM15,%YMM14      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM6,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM1,%YMM6                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM6,%YMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %YMM15,%YMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM11,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM11,%XMM11       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD %YMM12,%YMM1,%YMM6        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM8,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM15,%YMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM8,%XMM8         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD231PD %YMM12,%YMM1,%YMM15       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM8,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM11,%YMM8                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM8,%YMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD %YMM12,%YMM11,%YMM1       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM4,%YMM11                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM4,%XMM4         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVAPD %YMM1,%YMM8                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM0,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM0,%XMM0         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMULPD %YMM13,%YMM11,%YMM11           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD %YMM12,%YMM1,%YMM11       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM0,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM4,%YMM0                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM0,%YMM4             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %YMM1,%YMM0                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM10,%YMM1                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM10,%XMM10       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD %YMM12,%YMM4,%YMM0        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM10,%YMM4                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c70(%RIP),%YMM1,%YMM3  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM5,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM5,%XMM5         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM5,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c59(%RIP),%YMM4,%YMM9  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM4                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c4c(%RIP),%YMM1,%YMM14 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM7,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM2,%XMM2         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTF128 $0x1,%YMM7,%XMM7         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD231PD 0x2c33(%RIP),%YMM10,%YMM6 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM5                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c26(%RIP),%YMM4,%YMM11 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2c1d(%RIP),%YMM15,%YMM1 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM3,%XMM3                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM7,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM9,%XMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x2c07(%RIP),%YMM0,%YMM5  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM14,%XMM14               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM6,%XMM6                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2bf5(%RIP),%YMM15,%YMM8 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM1,%XMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM8,%XMM8                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM9,%YMM3,%YMM0    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM11,%XMM11               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM5,%XMM5                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM6,%YMM14,%YMM7   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM0,%YMM10               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTTPS2DQ %YMM7,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVINSERTF128 $0x1,%XMM8,%YMM1,%YMM0    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVINSERTF128 $0x1,%XMM5,%YMM11,%YMM14  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM14,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND 0x2bd6(%RIP),%YMM10,%YMM4       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVCVTTPS2DQ %YMM0,%YMM10               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND 0x2bca(%RIP),%YMM15,%YMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM2,%YMM4,%YMM3           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM3,%YMM9             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x2bb7(%RIP),%YMM6,%YMM15       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x2bcf(%RIP),%YMM9,%YMM3        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x2ba7(%RIP),%YMM10,%YMM7       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM15,%YMM7,%YMM4          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM4,%YMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x2bb4(%RIP),%YMM2,%YMM9        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM9,%YMM3,%YMM1           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM1,%YMM8             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2be0(%RIP),%YMM8,%YMM5      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERM2I128 $0,%YMM8,%YMM8,%YMM0       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPSHUFB 0x2c11(%RIP),%YMM8,%YMM7      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM5,%YMM14             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2ba2(%RIP),%YMM0,%YMM11     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $-0x7c,%YMM8,%YMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2bd3(%RIP),%YMM11,%YMM6     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM14,%YMM7,%YMM15              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2c05(%RIP),%YMM2,%YMM3      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVMOVDQA %YMM8,%YMM10                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2c17(%RIP),%YMM8,%YMM9      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM15,%YMM6,%YMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x2c2a(%RIP),%YMM1            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA %YMM4,-0x60(%RAX)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPBLENDVB %YMM1,%YMM9,%YMM3,%YMM8     | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 8-10    | 8\nVMOVDQA %YMM8,-0x40(%RAX)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPSHUFB 0x2c31(%RIP),%YMM10,%YMM0     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2c48(%RIP),%YMM10,%YMM5     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM0,%YMM11             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2c59(%RIP),%YMM10,%YMM6     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM11,%YMM5,%YMM14              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2c8b(%RIP),%YMM10,%YMM10    | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM6,%YMM7              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2c5c(%RIP),%YMM14,%YMM15    | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM7,%YMM10,%YMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM4,%YMM15,%YMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM2,-0x20(%RAX)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nCMP %RDX,%RAX                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nJNE 1270 <main+0x110>                 | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "1% of peak computational performance is used (0.38 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 39% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 417.00 to 171.00 cycles (2.44x speedup).",
        },
        {
          workaround = " - Reduce the number of FP multiply/FMA instructions\n - Reduce arithmetical operations on array elements\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)\n - execution of INT/FP operations in vector registers (the VPU is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 417.00 to 96.00 cycles (4.34x speedup).\n",
        },
      },
      potential = {
        {
          title = "FMA",
          txt = "Detected 64 FMA (fused multiply-add) operations.",
        },
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = " - VEXTRACTF128: 12 occurrences\n - VINSERTF128: 4 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 16 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision. In C/C++, FP constants are double precision by default and must be suffixed by 'f' to make them single precision.",
          details = " - VCVTDQ2PS (INT32 to FP32, SIMD): 12 occurrences\n - VCVTPD2PS (FP64 to FP32, SIMD): 8 occurrences\n - VCVTPS2PD (FP32 to FP64, SIMD): 24 occurrences\n - VCVTTPS2DQ (FP32 to INT32, SIMD): 4 occurrences\n - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "24 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 160 FP arithmetical operations:\n - 64: addition or subtraction (all inside FMA instructions)\n - 96: multiply (64 inside FMA instructions)\nThe binary loop is loading 1120 bytes (140 double precision FP elements).\nThe binary loop is storing 96 bytes (12 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.13 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 192\nloop length        : 1098\nused x86 registers : 2\nused mmx registers : 0\nused xmm registers : 14\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 0\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 69.00 cycles\ninstruction queue    : 96.00 cycles\ndecoding             : 96.00 cycles\nmicro-operation queue: 96.00 cycles\nfront end            : 96.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n------------------------------------------------------------\nuops   | 417.00 | 43.00 | 19.00 | 19.00 | 1.50 | 1.50 | 3.00\ncycles | 417.00 | 43.00 | 19.00 | 19.00 | 1.50 | 1.50 | 3.00\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 0.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 96.00\nDispatch  : 417.00\nData deps.: 0.00\nOverall L1: 417.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 100%\nload    : 100%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\nINT+FP\nall     : 100%\nload    : 100%\nstore   : 100%\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 40%\nload   : 50%\nstore  : 50%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 39%\nFP\nall     : 37%\nload    : 50%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 32%\nINT+FP\nall     : 39%\nload    : 50%\nstore   : 50%\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 37%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 417.00 cycles. At this rate:\n - 2% of peak load performance is reached (2.69 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 0% of peak store performance is reached (0.23 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1270\n\nInstruction                           | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n--------------------------------------------------------------------------------------------------------------------------\nVMOVDQA (%RAX),%YMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x60(%RAX),%RAX                   | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVDQA -0x40(%RAX),%YMM0             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA -0x20(%RAX),%YMM7             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSHUFB 0x2e55(%RIP),%YMM1,%YMM8      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2e4c(%RIP),%YMM0,%YMM14     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM8,%YMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2e3d(%RIP),%YMM7,%YMM5      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM14,%YMM15            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2e4e(%RIP),%YMM1,%YMM10     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM5,%YMM2              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2e3f(%RIP),%YMM0,%YMM3      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM9,%YMM10,%YMM11              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2e31(%RIP),%YMM7,%YMM4      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM15,%YMM3,%YMM6               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM2,%YMM4,%YMM7                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERM2I128 $0x21,%YMM6,%YMM11,%YMM1   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM11,%YMM7,%YMM10  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM7,%YMM6,%YMM8    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x6,%YMM1,%YMM6,%YMM0       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM10,%YMM11,%YMM11    | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERM2I128 $0x21,%YMM11,%YMM0,%YMM3   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM3,%YMM11,%YMM6      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM8,%YMM7,%YMM9       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $0x4e,%YMM6,%YMM7              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPERM2I128 $0x21,%YMM0,%YMM9,%YMM14   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM7,%YMM6,%YMM1       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x5,%YMM14,%YMM0,%YMM15     | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERM2I128 $0x21,%YMM9,%YMM11,%YMM5   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERMQ $0x4e,%YMM15,%YMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPALIGNR $0x5,%YMM5,%YMM9,%YMM4       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVEXTRACTI128 $0x1,%YMM1,%XMM0         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM1,%YMM8                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM0,%YMM10                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM4,%YMM7                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPALIGNR $0xb,%YMM15,%YMM2,%YMM11     | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVEXTRACTI128 $0x1,%YMM10,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM10,%YMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM4,%XMM4         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM5,%YMM2                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVZXBW %XMM4,%YMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM8,%YMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM8,%XMM14        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM7,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM3,%YMM8                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM14,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM2,%YMM0                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM1,%YMM3                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM7,%XMM14        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM1,%XMM2         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM11,%YMM1                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM15,%YMM6                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM14,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM1,%YMM14                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM2,%YMM4                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM14,%YMM14               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM11       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM15,%YMM5                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVZXBW %XMM11,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM3,%YMM7                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM15,%YMM11               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM1,%XMM3         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM4,%YMM2                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM3,%YMM4                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM4,%YMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM15,%YMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM3,%YMM4                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM14,%YMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PS %YMM9,%YMM9                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVEXTRACTF128 $0x1,%YMM14,%XMM14       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM11,%YMM11               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMULPD %YMM13,%YMM3,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM9,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM9,%XMM9         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM10,%YMM10               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFMADD231PD %YMM12,%YMM15,%YMM3       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM9,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM14,%YMM9                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM9,%YMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD %YMM12,%YMM14,%YMM15      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM1,%YMM14                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM1,%XMM1         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVAPD %YMM15,%YMM9                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM6,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM6,%XMM6         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMULPD %YMM13,%YMM14,%YMM14           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD %YMM12,%YMM15,%YMM14      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM6,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM1,%YMM6                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM6,%YMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %YMM15,%YMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM11,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM11,%XMM11       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD %YMM12,%YMM1,%YMM6        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM8,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM15,%YMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM8,%XMM8         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD231PD %YMM12,%YMM1,%YMM15       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM8,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM11,%YMM8                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM8,%YMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD %YMM12,%YMM11,%YMM1       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM4,%YMM11                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM4,%XMM4         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVAPD %YMM1,%YMM8                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM0,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM0,%XMM0         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMULPD %YMM13,%YMM11,%YMM11           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD %YMM12,%YMM1,%YMM11       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM0,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM4,%YMM0                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD %YMM13,%YMM0,%YMM4             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %YMM1,%YMM0                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTPS2PD %XMM10,%YMM1                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM10,%XMM10       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD %YMM12,%YMM4,%YMM0        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM10,%YMM4                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c70(%RIP),%YMM1,%YMM3  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM5,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM5,%XMM5         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM5,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c59(%RIP),%YMM4,%YMM9  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM4                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c4c(%RIP),%YMM1,%YMM14 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM7,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM2,%XMM2         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTF128 $0x1,%YMM7,%XMM7         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD231PD 0x2c33(%RIP),%YMM10,%YMM6 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM5                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2c26(%RIP),%YMM4,%YMM11 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2c1d(%RIP),%YMM15,%YMM1 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM3,%XMM3                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM7,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM9,%XMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x2c07(%RIP),%YMM0,%YMM5  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM14,%XMM14               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM6,%XMM6                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2bf5(%RIP),%YMM15,%YMM8 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM1,%XMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM8,%XMM8                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM9,%YMM3,%YMM0    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM11,%XMM11               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM5,%XMM5                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM6,%YMM14,%YMM7   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM0,%YMM10               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTTPS2DQ %YMM7,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVINSERTF128 $0x1,%XMM8,%YMM1,%YMM0    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVINSERTF128 $0x1,%XMM5,%YMM11,%YMM14  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM14,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND 0x2bd6(%RIP),%YMM10,%YMM4       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVCVTTPS2DQ %YMM0,%YMM10               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND 0x2bca(%RIP),%YMM15,%YMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM2,%YMM4,%YMM3           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM3,%YMM9             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x2bb7(%RIP),%YMM6,%YMM15       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x2bcf(%RIP),%YMM9,%YMM3        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x2ba7(%RIP),%YMM10,%YMM7       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM15,%YMM7,%YMM4          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM4,%YMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x2bb4(%RIP),%YMM2,%YMM9        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM9,%YMM3,%YMM1           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM1,%YMM8             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2be0(%RIP),%YMM8,%YMM5      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERM2I128 $0,%YMM8,%YMM8,%YMM0       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPSHUFB 0x2c11(%RIP),%YMM8,%YMM7      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM5,%YMM14             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2ba2(%RIP),%YMM0,%YMM11     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $-0x7c,%YMM8,%YMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2bd3(%RIP),%YMM11,%YMM6     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM14,%YMM7,%YMM15              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2c05(%RIP),%YMM2,%YMM3      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVMOVDQA %YMM8,%YMM10                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2c17(%RIP),%YMM8,%YMM9      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM15,%YMM6,%YMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x2c2a(%RIP),%YMM1            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA %YMM4,-0x60(%RAX)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPBLENDVB %YMM1,%YMM9,%YMM3,%YMM8     | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 8-10    | 8\nVMOVDQA %YMM8,-0x40(%RAX)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPSHUFB 0x2c31(%RIP),%YMM10,%YMM0     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2c48(%RIP),%YMM10,%YMM5     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM0,%YMM11             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2c59(%RIP),%YMM10,%YMM6     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM11,%YMM5,%YMM14              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2c8b(%RIP),%YMM10,%YMM10    | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM6,%YMM7              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2c5c(%RIP),%YMM14,%YMM15    | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM7,%YMM10,%YMM4               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM4,%YMM15,%YMM2               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM2,-0x20(%RAX)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nCMP %RDX,%RAX                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nJNE 1270 <main+0x110>                 | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "1% of peak computational performance is used (0.38 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 39% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 417.00 to 171.00 cycles (2.44x speedup).",
        },
        {
          workaround = " - Reduce the number of FP multiply/FMA instructions\n - Reduce arithmetical operations on array elements\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)\n - execution of INT/FP operations in vector registers (the VPU is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 417.00 to 96.00 cycles (4.34x speedup).\n",
        },
      },
      potential = {
        {
          title = "FMA",
          txt = "Detected 64 FMA (fused multiply-add) operations.",
        },
      },
    },
  common = {
    header = {
      "The loop is defined in /scratch/chps/users/user22024/Projet/AOC-Computer-vision/sobel.c:21-30.\n",
      "The related source loop is not unrolled or unrolled with no peel/tail loop.",
    },
    nb_paths = 1,
  },
}
