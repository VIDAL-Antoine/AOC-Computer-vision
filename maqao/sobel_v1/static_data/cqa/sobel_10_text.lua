_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 8 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = " - VINSERTF128: 4 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 4 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision. In C/C++, FP constants are double precision by default and must be suffixed by 'f' to make them single precision.",
          details = " - VCVTDQ2PD (INT32 to FP64, SIMD): 8 occurrences\n - VCVTPD2PS (FP64 to FP32, SIMD): 8 occurrences\n - VCVTTPS2DQ (FP32 to INT32, SIMD): 4 occurrences\n - VPACKUSDW: 4 occurrences\n - VPACKUSWB: 2 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "4 AVX instructions are processing arithmetic or math operations on single precision FP elements in vector mode (eight at a time).\n8 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 32 FP arithmetical operations:\n - 32: square root\nThe binary loop is loading 1256 bytes (157 double precision FP elements).\nThe binary loop is storing 480 bytes (60 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.02 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 237\nloop length        : 1321\nused x86 registers : 10\nused mmx registers : 0\nused xmm registers : 12\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 18\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 83.00 cycles\ninstruction queue    : 118.50 cycles\ndecoding             : 118.50 cycles\nmicro-operation queue: 119.00 cycles\nfront end            : 119.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n-------------------------------------------------------------\nuops   | 352.00 | 64.00 | 30.00 | 30.00 | 1.50 | 1.50 | 15.00\ncycles | 352.00 | 64.00 | 30.00 | 30.00 | 1.50 | 1.50 | 15.00\n\nCycles executing div or sqrt instructions: 128.00-136.00\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 119.00\nDispatch  : 352.00\nDIV/SQRT  : 128.00-136.00\nData deps.: 1.00\nOverall L1: 352.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : 100%\nadd-sub: 100%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 100%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 100%\nother   : 100%\nINT+FP\nall     : 100%\nload    : 100%\nstore   : 100%\nmul     : 100%\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 100%\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 39%\nload   : 45%\nstore  : 50%\nmul    : 50%\nadd-sub: 50%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 31%\nFP\nall     : 46%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 50%\nother   : 45%\nINT+FP\nall     : 40%\nload    : 45%\nstore   : 50%\nmul     : 50%\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 50%\nother   : 33%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 352.00 cycles. At this rate:\n - 2% of peak load performance is reached (3.57 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 2% of peak store performance is reached (1.36 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 2841\n\nInstruction                          | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n-------------------------------------------------------------------------------------------------------------------------\nVMOVDQA (%R12,%RAX,1),%YMM3          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW (%R12,%RAX,1),%YMM5        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R10,%RAX,1),%YMM2          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXWD %XMM5,%YMM14               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM3,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM5,%XMM12       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM7,%YMM8                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM2,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM12,%YMM13              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R10,%RAX,1),%YMM1        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM5,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R9,%RAX,1),%YMM2           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXWD %XMM8,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM8,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM3,%YMM12               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM3,%XMM8        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM6,%YMM0                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM2,%XMM3        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW (%R9,%RAX,1),%YMM6         | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM1,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM13,0x168(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM1,%YMM13               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM15,0x148(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM7,%YMM1                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM8,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM3,%YMM7                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM0,0x128(%RSP)            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPSUBW %YMM6,%YMM11,%YMM0            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSLLW $0x1,%YMM0,%YMM5              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVPSUBW %YMM7,%YMM11,%YMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x188(%RSP),%YMM6            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSLLW $0x1,%YMM8,%YMM2              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%R8,%RAX,1),%YMM0           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA %YMM15,0x108(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXBW (%R8,%RAX,1),%YMM15        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMULLW %YMM6,%YMM15,%YMM8           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVEXTRACTI128 $0x1,%YMM0,%XMM3        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQU (%RDI,%RAX,1),%YMM15         | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW %XMM3,%YMM7                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nMOV -0x40(%RSP),%RDX                 | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM0       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMULLW %YMM6,%YMM7,%YMM7            | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVPMOVZXBW %XMM0,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%RDI,%RAX,1),%YMM6        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM15,%YMM0               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM6,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM6,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM3,0xe8(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM6,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM0,0xc8(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM15,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%RDX,%RAX,1),%YMM15         | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA %YMM6,0xa8(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXBW %XMM15,%YMM0               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM6       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM6,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM0,%YMM6                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM6,0x88(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM15,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM6,0x68(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM15,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM6,0x48(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVSXWD %XMM5,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM5,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM14,%YMM13,%YMM6           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM0,%XMM0        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM6,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM8,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM0,%YMM0                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM8,%XMM8        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM6,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0xe8(%RSP),%YMM6,%YMM15       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0x88(%RSP),%YMM15,%YMM6       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0x168(%RSP),%YMM1,%YMM15      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM6,0x28(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVSXWD %XMM5,%YMM6                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD %YMM6,%YMM15,%YMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM8,%YMM5                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM5,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM2,%YMM5                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM2,%XMM2        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM3,%YMM6,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM0,%YMM15,%YMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0x148(%RSP),%YMM12,%YMM6      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM8,0x8(%RSP)              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPADDD %YMM5,%YMM6,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM7,%YMM8                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM7,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM8,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0xc8(%RSP),%YMM6,%YMM5        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0x68(%RSP),%YMM5,%YMM15       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM2,%YMM5                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVMOVDQA %YMM15,-0x18(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVDQA 0x108(%RSP),%YMM8            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nMOV -0x50(%RSP),%RDX                 | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVPSUBD 0x128(%RSP),%YMM8,%YMM6       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM5,%YMM6,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM7,%YMM8                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM8,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQU (%R11,%RAX,1),%YMM8          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBD 0xa8(%RSP),%YMM6,%YMM2        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM8,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD 0x48(%RSP),%YMM2,%YMM5        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXBW (%R11,%RAX,1),%YMM15       | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM6,%YMM2                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA 0x188(%RSP),%YMM8            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBW %YMM15,%YMM11,%YMM7           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM5,-0x38(%RSP)            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPSUBW %YMM2,%YMM11,%YMM5            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXBW (%RBX,%RAX,1),%YMM15       | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPSLLW $0x1,%YMM7,%YMM7              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%RBX,%RAX,1),%YMM2          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSLLW $0x1,%YMM5,%YMM5              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVPMULLW %YMM8,%YMM15,%YMM15          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVEXTRACTI128 $0x1,%YMM2,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM6,%YMM2                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMULLW %YMM8,%YMM2,%YMM6            | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVPMOVSXWD %XMM7,%YMM8                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM7,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM14,%YMM8,%YMM14           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM13,%YMM14,%YMM13          | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM15,%YMM8               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD 0xe8(%RSP),%YMM13,%YMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM8,%YMM2,%YMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM7,%YMM2                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPSUBD 0x168(%RSP),%YMM2,%YMM8       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM5,%YMM2                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPSUBD %YMM1,%YMM8,%YMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0x148(%RSP),%YMM2,%YMM8       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM3,%YMM1,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM12,%YMM8,%YMM12           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0x88(%RSP),%YMM14,%YMM13      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM15,%YMM14              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM14,%YMM3,%YMM7            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0xc8(%RSP),%YMM12,%YMM1       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM6,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM0,%YMM7,%YMM0             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM5,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM3,%YMM1,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM5,%YMM14               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD 0x68(%RSP),%YMM15,%YMM8       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x28(%RSP),%YMM15            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBD 0x128(%RSP),%YMM14,%YMM7      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM15,%YMM15,%YMM5          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPSUBD 0x108(%RSP),%YMM7,%YMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM6,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD 0xa8(%RSP),%YMM2,%YMM12       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM13,%YMM13,%YMM13         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPMOVZXWD %XMM6,%YMM1                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA 0x8(%RSP),%YMM7              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPADDD %YMM1,%YMM12,%YMM3            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM7,%YMM7,%YMM2            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD 0x48(%RSP),%YMM3,%YMM12       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA -0x18(%RSP),%YMM6            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMULLD %YMM0,%YMM0,%YMM0            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD %YMM13,%YMM5,%YMM14           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM6,%YMM6,%YMM1            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVMOVDQA -0x38(%RSP),%YMM5            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMULLD %YMM8,%YMM8,%YMM8            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD %YMM0,%YMM2,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM5,%YMM5,%YMM13           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPMULLD %YMM12,%YMM12,%YMM12         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD %YMM8,%YMM1,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTDQ2PD %XMM14,%YMM7               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM15,%YMM1               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTI128 $0x1,%YMM14,%XMM14      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM12,%YMM13,%YMM2           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTDQ2PD %XMM14,%YMM0               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM15,%YMM8               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM3,%YMM12               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVSQRTPD %YMM1,%YMM5                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVEXTRACTI128 $0x1,%YMM3,%XMM3        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PD %XMM2,%YMM1                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTI128 $0x1,%YMM2,%XMM2        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVSQRTPD %YMM7,%YMM6                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM0,%YMM13                 | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM8,%YMM7                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVCVTDQ2PD %XMM3,%YMM0                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM2,%YMM8                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVSQRTPD %YMM12,%YMM14                | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM8,%YMM2                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM0,%YMM15                 | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM1,%YMM3                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVCVTPD2PS %YMM6,%XMM6                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM3,%XMM1                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM13,%XMM13              | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM2,%XMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM5,%XMM5                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM7,%XMM7                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM14,%XMM14              | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM13,%YMM6,%YMM12 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM15,%XMM0               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM3,%YMM1,%YMM13  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCMPPS $0x2,%YMM10,%YMM12,%YMM8      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCMPPS $0x2,%YMM10,%YMM13,%YMM3      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVINSERTF128 $0x1,%XMM7,%YMM5,%YMM6   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM8,%YMM4,%YMM2              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCMPPS $0x2,%YMM10,%YMM6,%YMM5       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPAND %YMM3,%YMM4,%YMM8              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVINSERTF128 $0x1,%XMM0,%YMM14,%YMM15 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM5,%YMM4,%YMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCMPPS $0x2,%YMM10,%YMM15,%YMM1      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM7,%YMM2,%YMM14         | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM1,%YMM4,%YMM5              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERMQ $-0x28,%YMM14,%YMM0           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM12,%YMM12             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPACKUSDW %YMM8,%YMM5,%YMM2          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVCVTTPS2DQ %YMM6,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPERMQ $-0x28,%YMM2,%YMM14           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM15,%YMM15             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND %YMM0,%YMM9,%YMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTTPS2DQ %YMM13,%YMM13             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND %YMM14,%YMM9,%YMM0             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM0,%YMM7,%YMM1          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM12,%YMM4,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPAND %YMM6,%YMM4,%YMM5              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERMQ $-0x28,%YMM1,%YMM8            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM15,%YMM4,%YMM7             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM5,%YMM3,%YMM2          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM13,%YMM4,%YMM0             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERMQ $-0x28,%YMM2,%YMM14           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPACKUSDW %YMM0,%YMM7,%YMM1          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM14,%YMM9,%YMM6             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x1685(%RIP),%YMM14          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPERMQ $-0x28,%YMM1,%YMM12           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM12,%YMM9,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM3,%YMM6,%YMM5          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM5,%YMM2            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPBLENDVB %YMM8,%YMM2,%YMM14,%YMM8   | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 8-10    | 8\nVMOVDQU %YMM8,(%RDX,%RAX,1)          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nLEA 0x20(%RAX),%RAX                  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nCMP %RAX,-0x48(%RSP)                 | 1     | 0    | 0    | 1    | 0    | 0.50 | 0.50 | 0  | 1       | 1\nJNE 2841 <sobel_v1+0x2b1>            | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.09 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 40% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 352.00 to 150.00 cycles (2.35x speedup).",
        },
        {
          workaround = " -  - Reduce the number of division or square root instructions:\n  * If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast\n - Check whether you really need double precision. If not, switch to single precision to speedup execution\n - Reduce arithmetical operations on array elements\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of divide and square root operations (the divide/square root unit is a bottleneck)\n - execution of INT/FP operations in vector registers (the VPU is a bottleneck)\n",
        },
      },
      potential = {
        {
          title = "Expensive FP math instructions/calls",
          txt = "Detected performance impact from expensive FP math instructions/calls.\nBy removing/reexpressing them, you can lower the cost of an iteration from 352.00 to 208.00 cycles (1.69x speedup).",
        },
        {
          details = " - VCVTDQ2PD: 8 occurrences\n - VCVTPD2PS: 8 occurrences\n - VCVTTPS2DQ: 4 occurrences\n - VEXTRACTI128: 28 occurrences\n - VINSERTF128: 4 occurrences\n - VPACKUSDW: 4 occurrences\n - VPACKUSWB: 2 occurrences\n - VPBLENDVB: 1 occurrences\n - VPERMQ: 6 occurrences\n - VPMOVSXWD: 8 occurrences\n - VPMOVZXBW: 16 occurrences\n - VPMOVZXWD: 24 occurrences\n - VPSLLW: 4 occurrences\n",
          title = "Special instructions executing on a single port",
          txt = "2.05x slowdown from special instructions executing on a single port.\n",
        },
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 8 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = " - VINSERTF128: 4 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 4 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision. In C/C++, FP constants are double precision by default and must be suffixed by 'f' to make them single precision.",
          details = " - VCVTDQ2PD (INT32 to FP64, SIMD): 8 occurrences\n - VCVTPD2PS (FP64 to FP32, SIMD): 8 occurrences\n - VCVTTPS2DQ (FP32 to INT32, SIMD): 4 occurrences\n - VPACKUSDW: 4 occurrences\n - VPACKUSWB: 2 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "4 AVX instructions are processing arithmetic or math operations on single precision FP elements in vector mode (eight at a time).\n8 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 32 FP arithmetical operations:\n - 32: square root\nThe binary loop is loading 1256 bytes (157 double precision FP elements).\nThe binary loop is storing 480 bytes (60 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.02 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 237\nloop length        : 1321\nused x86 registers : 10\nused mmx registers : 0\nused xmm registers : 12\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 18\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 83.00 cycles\ninstruction queue    : 118.50 cycles\ndecoding             : 118.50 cycles\nmicro-operation queue: 119.00 cycles\nfront end            : 119.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n-------------------------------------------------------------\nuops   | 352.00 | 64.00 | 30.00 | 30.00 | 1.50 | 1.50 | 15.00\ncycles | 352.00 | 64.00 | 30.00 | 30.00 | 1.50 | 1.50 | 15.00\n\nCycles executing div or sqrt instructions: 128.00-136.00\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 119.00\nDispatch  : 352.00\nDIV/SQRT  : 128.00-136.00\nData deps.: 1.00\nOverall L1: 352.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : 100%\nadd-sub: 100%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 100%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 100%\nother   : 100%\nINT+FP\nall     : 100%\nload    : 100%\nstore   : 100%\nmul     : 100%\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 100%\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 39%\nload   : 45%\nstore  : 50%\nmul    : 50%\nadd-sub: 50%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 31%\nFP\nall     : 46%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 50%\nother   : 45%\nINT+FP\nall     : 40%\nload    : 45%\nstore   : 50%\nmul     : 50%\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: 50%\nother   : 33%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 352.00 cycles. At this rate:\n - 2% of peak load performance is reached (3.57 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 2% of peak store performance is reached (1.36 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 2841\n\nInstruction                          | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n-------------------------------------------------------------------------------------------------------------------------\nVMOVDQA (%R12,%RAX,1),%YMM3          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW (%R12,%RAX,1),%YMM5        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R10,%RAX,1),%YMM2          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXWD %XMM5,%YMM14               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM3,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM5,%XMM12       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM7,%YMM8                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM2,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM12,%YMM13              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R10,%RAX,1),%YMM1        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM5,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R9,%RAX,1),%YMM2           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXWD %XMM8,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM8,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM3,%YMM12               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM3,%XMM8        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM6,%YMM0                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM2,%XMM3        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW (%R9,%RAX,1),%YMM6         | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM1,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM13,0x168(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM1,%YMM13               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM15,0x148(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM7,%YMM1                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM8,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM3,%YMM7                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM0,0x128(%RSP)            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPSUBW %YMM6,%YMM11,%YMM0            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSLLW $0x1,%YMM0,%YMM5              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVPSUBW %YMM7,%YMM11,%YMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x188(%RSP),%YMM6            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSLLW $0x1,%YMM8,%YMM2              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%R8,%RAX,1),%YMM0           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA %YMM15,0x108(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXBW (%R8,%RAX,1),%YMM15        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMULLW %YMM6,%YMM15,%YMM8           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVEXTRACTI128 $0x1,%YMM0,%XMM3        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQU (%RDI,%RAX,1),%YMM15         | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW %XMM3,%YMM7                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nMOV -0x40(%RSP),%RDX                 | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM0       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMULLW %YMM6,%YMM7,%YMM7            | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVPMOVZXBW %XMM0,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%RDI,%RAX,1),%YMM6        | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM15,%YMM0               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM6,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM6,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM3,0xe8(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM6,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM0,0xc8(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM15,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%RDX,%RAX,1),%YMM15         | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQA %YMM6,0xa8(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXBW %XMM15,%YMM0               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM6       | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM6,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM0,%YMM6                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM6,0x88(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM15,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM6,0x68(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM15,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM6,0x48(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVSXWD %XMM5,%YMM15               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM5,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM14,%YMM13,%YMM6           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM0,%XMM0        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM6,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM8,%YMM15               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM0,%YMM0                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM8,%XMM8        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM6,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0xe8(%RSP),%YMM6,%YMM15       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0x88(%RSP),%YMM15,%YMM6       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0x168(%RSP),%YMM1,%YMM15      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM6,0x28(%RSP)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVSXWD %XMM5,%YMM6                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD %YMM6,%YMM15,%YMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM8,%YMM5                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM5,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM2,%YMM5                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM2,%XMM2        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM3,%YMM6,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM0,%YMM15,%YMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0x148(%RSP),%YMM12,%YMM6      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM8,0x8(%RSP)              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPADDD %YMM5,%YMM6,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM7,%YMM8                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM7,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM8,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0xc8(%RSP),%YMM6,%YMM5        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0x68(%RSP),%YMM5,%YMM15       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM2,%YMM5                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVMOVDQA %YMM15,-0x18(%RSP)           | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVDQA 0x108(%RSP),%YMM8            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nMOV -0x50(%RSP),%RDX                 | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVPSUBD 0x128(%RSP),%YMM8,%YMM6       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM5,%YMM6,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM7,%YMM8                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM8,%YMM15,%YMM6            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQU (%R11,%RAX,1),%YMM8          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBD 0xa8(%RSP),%YMM6,%YMM2        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM8,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD 0x48(%RSP),%YMM2,%YMM5        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXBW (%R11,%RAX,1),%YMM15       | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM6,%YMM2                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA 0x188(%RSP),%YMM8            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBW %YMM15,%YMM11,%YMM7           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM5,-0x38(%RSP)            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPSUBW %YMM2,%YMM11,%YMM5            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXBW (%RBX,%RAX,1),%YMM15       | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPSLLW $0x1,%YMM7,%YMM7              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%RBX,%RAX,1),%YMM2          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSLLW $0x1,%YMM5,%YMM5              | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVPMULLW %YMM8,%YMM15,%YMM15          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVEXTRACTI128 $0x1,%YMM2,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM6,%YMM2                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMULLW %YMM8,%YMM2,%YMM6            | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 16      | 9\nVPMOVSXWD %XMM7,%YMM8                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM7,%XMM7        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM14,%YMM8,%YMM14           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM13,%YMM14,%YMM13          | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM15,%YMM8               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD 0xe8(%RSP),%YMM13,%YMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM8,%YMM2,%YMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM7,%YMM2                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPSUBD 0x168(%RSP),%YMM2,%YMM8       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM5,%YMM2                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPSUBD %YMM1,%YMM8,%YMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD 0x148(%RSP),%YMM2,%YMM8       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM3,%YMM1,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM12,%YMM8,%YMM12           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0x88(%RSP),%YMM14,%YMM13      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM15,%YMM14              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM14,%YMM3,%YMM7            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD 0xc8(%RSP),%YMM12,%YMM1       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXWD %XMM6,%YMM3                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPADDD %YMM0,%YMM7,%YMM0             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM5,%XMM5        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM3,%YMM1,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM5,%YMM14               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD 0x68(%RSP),%YMM15,%YMM8       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x28(%RSP),%YMM15            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBD 0x128(%RSP),%YMM14,%YMM7      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM15,%YMM15,%YMM5          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPSUBD 0x108(%RSP),%YMM7,%YMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM6,%XMM6        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD 0xa8(%RSP),%YMM2,%YMM12       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM13,%YMM13,%YMM13         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPMOVZXWD %XMM6,%YMM1                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA 0x8(%RSP),%YMM7              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPADDD %YMM1,%YMM12,%YMM3            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM7,%YMM7,%YMM2            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD 0x48(%RSP),%YMM3,%YMM12       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA -0x18(%RSP),%YMM6            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMULLD %YMM0,%YMM0,%YMM0            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD %YMM13,%YMM5,%YMM14           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM6,%YMM6,%YMM1            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVMOVDQA -0x38(%RSP),%YMM5            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMULLD %YMM8,%YMM8,%YMM8            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD %YMM0,%YMM2,%YMM15            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMULLD %YMM5,%YMM5,%YMM13           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPMULLD %YMM12,%YMM12,%YMM12         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 1\nVPADDD %YMM8,%YMM1,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTDQ2PD %XMM14,%YMM7               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM15,%YMM1               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTI128 $0x1,%YMM14,%XMM14      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM15      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM12,%YMM13,%YMM2           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTDQ2PD %XMM14,%YMM0               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM15,%YMM8               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM3,%YMM12               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVSQRTPD %YMM1,%YMM5                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVEXTRACTI128 $0x1,%YMM3,%XMM3        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PD %XMM2,%YMM1                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTI128 $0x1,%YMM2,%XMM2        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVSQRTPD %YMM7,%YMM6                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM0,%YMM13                 | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM8,%YMM7                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVCVTDQ2PD %XMM3,%YMM0                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PD %XMM2,%YMM8                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVSQRTPD %YMM12,%YMM14                | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM8,%YMM2                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM0,%YMM15                 | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVSQRTPD %YMM1,%YMM3                  | 18    | 18   | 0    | 0    | 0    | 0    | 0    | 0  | 37      | 16-17\nVCVTPD2PS %YMM6,%XMM6                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM3,%XMM1                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM13,%XMM13              | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM2,%XMM3                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM5,%XMM5                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM7,%XMM7                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM14,%XMM14              | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM13,%YMM6,%YMM12 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM15,%XMM0               | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM3,%YMM1,%YMM13  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCMPPS $0x2,%YMM10,%YMM12,%YMM8      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCMPPS $0x2,%YMM10,%YMM13,%YMM3      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVINSERTF128 $0x1,%XMM7,%YMM5,%YMM6   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM8,%YMM4,%YMM2              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCMPPS $0x2,%YMM10,%YMM6,%YMM5       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPAND %YMM3,%YMM4,%YMM8              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVINSERTF128 $0x1,%XMM0,%YMM14,%YMM15 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM5,%YMM4,%YMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCMPPS $0x2,%YMM10,%YMM15,%YMM1      | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM7,%YMM2,%YMM14         | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM1,%YMM4,%YMM5              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERMQ $-0x28,%YMM14,%YMM0           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM12,%YMM12             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPACKUSDW %YMM8,%YMM5,%YMM2          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVCVTTPS2DQ %YMM6,%YMM6               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPERMQ $-0x28,%YMM2,%YMM14           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM15,%YMM15             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND %YMM0,%YMM9,%YMM7              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVCVTTPS2DQ %YMM13,%YMM13             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND %YMM14,%YMM9,%YMM0             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM0,%YMM7,%YMM1          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM12,%YMM4,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPAND %YMM6,%YMM4,%YMM5              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERMQ $-0x28,%YMM1,%YMM8            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM15,%YMM4,%YMM7             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM5,%YMM3,%YMM2          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM13,%YMM4,%YMM0             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERMQ $-0x28,%YMM2,%YMM14           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPACKUSDW %YMM0,%YMM7,%YMM1          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPAND %YMM14,%YMM9,%YMM6             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x1685(%RIP),%YMM14          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPERMQ $-0x28,%YMM1,%YMM12           | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND %YMM12,%YMM9,%YMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM3,%YMM6,%YMM5          | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM5,%YMM2            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPBLENDVB %YMM8,%YMM2,%YMM14,%YMM8   | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 8-10    | 8\nVMOVDQU %YMM8,(%RDX,%RAX,1)          | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nLEA 0x20(%RAX),%RAX                  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nCMP %RAX,-0x48(%RSP)                 | 1     | 0    | 0    | 1    | 0    | 0.50 | 0.50 | 0  | 1       | 1\nJNE 2841 <sobel_v1+0x2b1>            | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.09 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 40% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 352.00 to 150.00 cycles (2.35x speedup).",
        },
        {
          workaround = " -  - Reduce the number of division or square root instructions:\n  * If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast\n - Check whether you really need double precision. If not, switch to single precision to speedup execution\n - Reduce arithmetical operations on array elements\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of divide and square root operations (the divide/square root unit is a bottleneck)\n - execution of INT/FP operations in vector registers (the VPU is a bottleneck)\n",
        },
      },
      potential = {
        {
          title = "Expensive FP math instructions/calls",
          txt = "Detected performance impact from expensive FP math instructions/calls.\nBy removing/reexpressing them, you can lower the cost of an iteration from 352.00 to 208.00 cycles (1.69x speedup).",
        },
        {
          details = " - VCVTDQ2PD: 8 occurrences\n - VCVTPD2PS: 8 occurrences\n - VCVTTPS2DQ: 4 occurrences\n - VEXTRACTI128: 28 occurrences\n - VINSERTF128: 4 occurrences\n - VPACKUSDW: 4 occurrences\n - VPACKUSWB: 2 occurrences\n - VPBLENDVB: 1 occurrences\n - VPERMQ: 6 occurrences\n - VPMOVSXWD: 8 occurrences\n - VPMOVZXBW: 16 occurrences\n - VPMOVZXWD: 24 occurrences\n - VPSLLW: 4 occurrences\n",
          title = "Special instructions executing on a single port",
          txt = "2.05x slowdown from special instructions executing on a single port.\n",
        },
      },
    },
  common = {
    header = {
      "The loop is defined in /scratch/chps/users/user22024/Projet/AOC-Computer-vision/sobel.c:113,134-141.\n",
      "It is main loop of related source loop which is unrolled by 32 (including vectorization).",
    },
    nb_paths = 1,
  },
}
