_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 2 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = " - VEXTRACTF128: 12 occurrences\n - VINSERTF128: 4 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 16 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision. In C/C++, FP constants are double precision by default and must be suffixed by 'f' to make them single precision.",
          details = " - VCVTDQ2PS (INT32 to FP32, SIMD): 12 occurrences\n - VCVTPD2PS (FP64 to FP32, SIMD): 8 occurrences\n - VCVTPS2PD (FP32 to FP64, SIMD): 24 occurrences\n - VCVTTPS2DQ (FP32 to INT32, SIMD): 4 occurrences\n - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "24 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 160 FP arithmetical operations:\n - 64: addition or subtraction (all inside FMA instructions)\n - 96: multiply (64 inside FMA instructions)\nThe binary loop is loading 1632 bytes (204 double precision FP elements).\nThe binary loop is storing 96 bytes (12 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.09 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 190\nloop length        : 1153\nused x86 registers : 3\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 0\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 73.00 cycles\ninstruction queue    : 95.00 cycles\ndecoding             : 95.00 cycles\nmicro-operation queue: 95.00 cycles\nfront end            : 95.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n------------------------------------------------------------\nuops   | 417.00 | 40.00 | 27.00 | 27.00 | 2.00 | 2.00 | 3.00\ncycles | 417.00 | 40.00 | 27.00 | 27.00 | 2.00 | 2.00 | 3.00\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 95.00\nDispatch  : 417.00\nData deps.: 1.00\nOverall L1: 417.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 100%\nload    : 100%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\nINT+FP\nall     : 100%\nload    : 100%\nstore   : 100%\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 40%\nload   : 50%\nstore  : 50%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 39%\nFP\nall     : 37%\nload    : 50%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 31%\nINT+FP\nall     : 38%\nload    : 50%\nstore   : 50%\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 36%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 417.00 cycles. At this rate:\n - 3% of peak load performance is reached (3.91 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 0% of peak store performance is reached (0.23 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1660\n\nInstruction                            | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n---------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%R10),%YMM3                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x60(%R10),%R10                    | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVDQU -0x40(%R10),%YMM0              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x60(%R11),%R11                    | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVDQU -0x20(%R10),%YMM4              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nCMP %R10,%R9                           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVPSHUFB 0x2a5b(%RIP),%YMM3,%YMM2       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2a52(%RIP),%YMM0,%YMM8       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM2,%YMM1               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2a43(%RIP),%YMM4,%YMM12      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM8,%YMM9               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2a54(%RIP),%YMM3,%YMM6       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM12,%YMM13             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2a45(%RIP),%YMM0,%YMM10      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM1,%YMM6,%YMM7                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2a38(%RIP),%YMM4,%YMM14      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM9,%YMM10,%YMM11               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM13,%YMM14,%YMM15              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERM2I128 $0x21,%YMM11,%YMM7,%YMM5    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM7,%YMM15,%YMM2    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM15,%YMM11,%YMM3   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x6,%YMM5,%YMM11,%YMM0       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM2,%YMM7,%YMM1        | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM3,%YMM15,%YMM4       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERM2I128 $0x21,%YMM1,%YMM0,%YMM8     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM0,%YMM4,%YMM6     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM8,%YMM1,%YMM9        | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x5,%YMM6,%YMM0,%YMM7        | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $0x4e,%YMM9,%YMM14              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPERM2I128 $0x21,%YMM4,%YMM1,%YMM10    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM14,%YMM9,%YMM15      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x5,%YMM10,%YMM4,%YMM11      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $0x4e,%YMM7,%YMM12              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM15,%YMM5                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPALIGNR $0xb,%YMM7,%YMM12,%YMM13      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPMOVZXBW %XMM11,%YMM10                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM5,%YMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM15,%XMM0         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM10,%YMM14                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM5,%XMM2          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM0,%YMM3                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM11        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM2,%YMM1                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVZXBW %XMM11,%YMM12                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM3,%YMM7                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM10,%XMM15        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM13,%YMM10                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM4,%YMM6                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM12,%YMM2                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM1,%YMM4                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVEXTRACTI128 $0x1,%YMM3,%XMM8          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM15,%YMM3                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM12,%XMM1         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM10,%YMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM13,%XMM13        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM8,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM14,%YMM8                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVZXBW %XMM13,%YMM11                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM12,%YMM14                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTDQ2PS %YMM9,%YMM0                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM1,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM10,%XMM15        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM7,%YMM5                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM15,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM2,%YMM7                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM14,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PS %YMM9,%YMM2                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM11,%YMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTF128 $0x1,%YMM14,%XMM14        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM9,%YMM10                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM14,%YMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTI128 $0x1,%YMM11,%XMM11        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMULPD 0x2922(%RIP),%YMM15,%YMM15      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVPMOVSXWD %XMM11,%YMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM1,%YMM13                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTDQ2PS %YMM12,%YMM11                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM6,%YMM1                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x2908(%RIP),%YMM9,%YMM12       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM6,%XMM6          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM6,%YMM6                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2915(%RIP),%YMM1,%YMM15  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM13,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM13,%XMM13        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM4,%YMM14                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x28fd(%RIP),%YMM12,%YMM6  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM13,%YMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x28d0(%RIP),%YMM1,%YMM9        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM4,%XMM4          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM4,%YMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x28be(%RIP),%YMM12,%YMM1       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM10,%YMM13                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x28d0(%RIP),%YMM9,%YMM14  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM10,%XMM10        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM5,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x28bd(%RIP),%YMM1,%YMM4   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM10,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x2890(%RIP),%YMM13,%YMM12      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM5,%XMM5          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM5,%YMM5                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMOVAPD %YMM9,%YMM13                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMULPD 0x2879(%RIP),%YMM1,%YMM9        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2890(%RIP),%YMM12,%YMM13 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM0,%YMM12                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM0,%XMM0          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM3,%YMM3                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFMADD132PD 0x2879(%RIP),%YMM9,%YMM5   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM0,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM8,%YMM0                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM8,%XMM8          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM11,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM11,%XMM11        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD 0x2876(%RIP),%YMM15,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM8,%YMM15                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x2829(%RIP),%YMM10,%YMM1       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2860(%RIP),%YMM6,%YMM15  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM3,%YMM6                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM3,%XMM3          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM11,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x2828(%RIP),%YMM1,%YMM12  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM0,%XMM1                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x283b(%RIP),%YMM14,%YMM6  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM3,%YMM14                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM6,%XMM0                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x27eb(%RIP),%YMM10,%YMM11      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2822(%RIP),%YMM4,%YMM14  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM7,%YMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM7,%XMM7          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM14,%XMM8                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x27ea(%RIP),%YMM9,%YMM11  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM15,%XMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x27fc(%RIP),%YMM13,%YMM4  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM7,%YMM13                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM4,%XMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM8,%YMM0,%YMM15    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM15,%YMM6                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVINSERTF128 $0x1,%XMM9,%YMM1,%YMM10    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD 0x27da(%RIP),%YMM5,%YMM13  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM5                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM2,%XMM2          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM13,%XMM13                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x27c2(%RIP),%YMM12,%YMM5  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM12                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM5,%XMM5                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM13,%YMM4,%YMM2    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM2,%YMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFMADD132PD 0x27a7(%RIP),%YMM11,%YMM12 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM12,%XMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTTPS2DQ %YMM10,%YMM11               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND 0x27b5(%RIP),%YMM6,%YMM3         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x27ad(%RIP),%YMM1,%YMM0         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVINSERTF128 $0x1,%XMM12,%YMM5,%YMM10   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x279f(%RIP),%YMM11,%YMM14       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVCVTTPS2DQ %YMM10,%YMM11               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPACKUSDW %YMM3,%YMM14,%YMM7           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM7,%YMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x27a7(%RIP),%YMM9,%YMM14        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x277f(%RIP),%YMM11,%YMM8        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM8,%YMM0,%YMM15           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM15,%YMM6             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x278c(%RIP),%YMM6,%YMM3         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM3,%YMM14,%YMM7           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVMOVDQA 0x2860(%RIP),%YMM14            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPERMQ $-0x28,%YMM7,%YMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM9,%YMM13                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERM2I128 $0,%YMM9,%YMM9,%YMM4        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPSHUFB 0x27a6(%RIP),%YMM9,%YMM1       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $-0x7c,%YMM9,%YMM8              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2857(%RIP),%YMM9,%YMM7       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM1,%YMM5               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2768(%RIP),%YMM4,%YMM2       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x27bf(%RIP),%YMM9,%YMM10      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x27f6(%RIP),%YMM9,%YMM6       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM7,%YMM9               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2847(%RIP),%YMM13,%YMM4      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM5,%YMM10,%YMM11               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x285a(%RIP),%YMM13,%YMM1      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2771(%RIP),%YMM2,%YMM12      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM1,%YMM5               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPOR %YMM9,%YMM4,%YMM2                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM11,%YMM12,%YMM0               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2878(%RIP),%YMM13,%YMM13     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x278f(%RIP),%YMM8,%YMM15      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM5,%YMM13,%YMM10               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2842(%RIP),%YMM2,%YMM12      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPBLENDVB %YMM14,%YMM6,%YMM15,%YMM3    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 8-10    | 8\nVMOVDQU %YMM0,-0x60(%R11)              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPOR %YMM10,%YMM12,%YMM11              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQU %YMM3,-0x40(%R11)              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVDQU %YMM11,-0x20(%R11)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nJNE 1660 <main._omp_fn.0+0xc0>         | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "1% of peak computational performance is used (0.38 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 38% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 417.00 to 171.00 cycles (2.44x speedup).",
        },
        {
          workaround = " - Reduce the number of FP multiply/FMA instructions\n - Reduce arithmetical operations on array elements\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)\n - execution of INT/FP operations in vector registers (the VPU is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 417.00 to 95.00 cycles (4.39x speedup).\n",
        },
      },
      potential = {
        {
          title = "FMA",
          txt = "Detected 64 FMA (fused multiply-add) operations.",
        },
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 2 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = " - VEXTRACTF128: 12 occurrences\n - VINSERTF128: 4 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 16 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements. Use double instead of single precision only when/where needed by numerical stability and avoid mixing precision. In C/C++, FP constants are double precision by default and must be suffixed by 'f' to make them single precision.",
          details = " - VCVTDQ2PS (INT32 to FP32, SIMD): 12 occurrences\n - VCVTPD2PS (FP64 to FP32, SIMD): 8 occurrences\n - VCVTPS2PD (FP32 to FP64, SIMD): 24 occurrences\n - VCVTTPS2DQ (FP32 to INT32, SIMD): 4 occurrences\n - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "24 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 160 FP arithmetical operations:\n - 64: addition or subtraction (all inside FMA instructions)\n - 96: multiply (64 inside FMA instructions)\nThe binary loop is loading 1632 bytes (204 double precision FP elements).\nThe binary loop is storing 96 bytes (12 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.09 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 190\nloop length        : 1153\nused x86 registers : 3\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 0\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 73.00 cycles\ninstruction queue    : 95.00 cycles\ndecoding             : 95.00 cycles\nmicro-operation queue: 95.00 cycles\nfront end            : 95.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n------------------------------------------------------------\nuops   | 417.00 | 40.00 | 27.00 | 27.00 | 2.00 | 2.00 | 3.00\ncycles | 417.00 | 40.00 | 27.00 | 27.00 | 2.00 | 2.00 | 3.00\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 95.00\nDispatch  : 417.00\nData deps.: 1.00\nOverall L1: 417.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 100%\nload   : 100%\nstore  : 100%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 100%\nFP\nall     : 100%\nload    : 100%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\nINT+FP\nall     : 100%\nload    : 100%\nstore   : 100%\nmul     : 100%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 100%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 40%\nload   : 50%\nstore  : 50%\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: NA (no add-sub vectorizable/vectorized instructions)\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 39%\nFP\nall     : 37%\nload    : 50%\nstore   : NA (no store vectorizable/vectorized instructions)\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 31%\nINT+FP\nall     : 38%\nload    : 50%\nstore   : 50%\nmul     : 50%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 50%\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 36%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 417.00 cycles. At this rate:\n - 3% of peak load performance is reached (3.91 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 0% of peak store performance is reached (0.23 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1660\n\nInstruction                            | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n---------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%R10),%YMM3                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x60(%R10),%R10                    | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVDQU -0x40(%R10),%YMM0              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x60(%R11),%R11                    | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVDQU -0x20(%R10),%YMM4              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nCMP %R10,%R9                           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVPSHUFB 0x2a5b(%RIP),%YMM3,%YMM2       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2a52(%RIP),%YMM0,%YMM8       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM2,%YMM1               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2a43(%RIP),%YMM4,%YMM12      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM8,%YMM9               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2a54(%RIP),%YMM3,%YMM6       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM12,%YMM13             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2a45(%RIP),%YMM0,%YMM10      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM1,%YMM6,%YMM7                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2a38(%RIP),%YMM4,%YMM14      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM9,%YMM10,%YMM11               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM13,%YMM14,%YMM15              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERM2I128 $0x21,%YMM11,%YMM7,%YMM5    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM7,%YMM15,%YMM2    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM15,%YMM11,%YMM3   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x6,%YMM5,%YMM11,%YMM0       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM2,%YMM7,%YMM1        | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x6,%YMM3,%YMM15,%YMM4       | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERM2I128 $0x21,%YMM1,%YMM0,%YMM8     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPERM2I128 $0x21,%YMM0,%YMM4,%YMM6     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM8,%YMM1,%YMM9        | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x5,%YMM6,%YMM0,%YMM7        | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $0x4e,%YMM9,%YMM14              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPERM2I128 $0x21,%YMM4,%YMM1,%YMM10    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPALIGNR $0x5,%YMM14,%YMM9,%YMM15      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPALIGNR $0x5,%YMM10,%YMM4,%YMM11      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $0x4e,%YMM7,%YMM12              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM15,%YMM5                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPALIGNR $0xb,%YMM7,%YMM12,%YMM13      | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPMOVZXBW %XMM11,%YMM10                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM5,%YMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM15,%XMM0         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM10,%YMM14                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM5,%XMM2          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM0,%YMM3                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM11        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM2,%YMM1                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVZXBW %XMM11,%YMM12                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM3,%YMM7                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM10,%XMM15        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM13,%YMM10                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM4,%YMM6                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM12,%YMM2                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM1,%YMM4                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVEXTRACTI128 $0x1,%YMM3,%XMM8          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM15,%YMM3                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM12,%XMM1         | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM10,%YMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM13,%XMM13        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM8,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM14,%YMM8                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVZXBW %XMM13,%YMM11                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVCVTDQ2PS %YMM12,%YMM14                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTDQ2PS %YMM9,%YMM0                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM1,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM10,%XMM15        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM7,%YMM5                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM15,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM2,%YMM7                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM14,%YMM15                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTDQ2PS %YMM9,%YMM2                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPMOVSXWD %XMM11,%YMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTF128 $0x1,%YMM14,%XMM14        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM9,%YMM10                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM14,%YMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTI128 $0x1,%YMM11,%XMM11        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMULPD 0x2922(%RIP),%YMM15,%YMM15      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVPMOVSXWD %XMM11,%YMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVCVTDQ2PS %YMM1,%YMM13                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTDQ2PS %YMM12,%YMM11                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVCVTPS2PD %XMM6,%YMM1                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x2908(%RIP),%YMM9,%YMM12       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM6,%XMM6          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM6,%YMM6                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x2915(%RIP),%YMM1,%YMM15  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM13,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM13,%XMM13        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM4,%YMM14                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x28fd(%RIP),%YMM12,%YMM6  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM13,%YMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x28d0(%RIP),%YMM1,%YMM9        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM4,%XMM4          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM4,%YMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x28be(%RIP),%YMM12,%YMM1       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM10,%YMM13                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x28d0(%RIP),%YMM9,%YMM14  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM10,%XMM10        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM5,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x28bd(%RIP),%YMM1,%YMM4   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM10,%YMM1                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x2890(%RIP),%YMM13,%YMM12      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVEXTRACTF128 $0x1,%YMM5,%XMM5          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM5,%YMM5                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMOVAPD %YMM9,%YMM13                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMULPD 0x2879(%RIP),%YMM1,%YMM9        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2890(%RIP),%YMM12,%YMM13 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM0,%YMM12                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM0,%XMM0          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTDQ2PS %YMM3,%YMM3                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFMADD132PD 0x2879(%RIP),%YMM9,%YMM5   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM0,%YMM9                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPS2PD %XMM8,%YMM0                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM8,%XMM8          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM11,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM11,%XMM11        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD 0x2876(%RIP),%YMM15,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM8,%YMM15                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x2829(%RIP),%YMM10,%YMM1       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2860(%RIP),%YMM6,%YMM15  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM3,%YMM6                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM3,%XMM3          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPS2PD %XMM11,%YMM10                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x2828(%RIP),%YMM1,%YMM12  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM0,%XMM1                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x283b(%RIP),%YMM14,%YMM6  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM3,%YMM14                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM6,%XMM0                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVMULPD 0x27eb(%RIP),%YMM10,%YMM11      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0x2822(%RIP),%YMM4,%YMM14  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM7,%YMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM7,%XMM7          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM14,%XMM8                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD231PD 0x27ea(%RIP),%YMM9,%YMM11  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM15,%XMM9                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x27fc(%RIP),%YMM13,%YMM4  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM7,%YMM13                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM4,%XMM4                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM8,%YMM0,%YMM15    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM15,%YMM6                | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVINSERTF128 $0x1,%XMM9,%YMM1,%YMM10    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVFMADD132PD 0x27da(%RIP),%YMM5,%YMM13  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM5                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVEXTRACTF128 $0x1,%YMM2,%XMM2          | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTPD2PS %YMM13,%XMM13                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVFMADD132PD 0x27c2(%RIP),%YMM12,%YMM5  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPS2PD %XMM2,%YMM12                 | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTPD2PS %YMM5,%XMM5                  | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVINSERTF128 $0x1,%XMM13,%YMM4,%YMM2    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVCVTTPS2DQ %YMM2,%YMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFMADD132PD 0x27a7(%RIP),%YMM11,%YMM12 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVCVTPD2PS %YMM12,%XMM12                | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 7       | 7\nVCVTTPS2DQ %YMM10,%YMM11               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPAND 0x27b5(%RIP),%YMM6,%YMM3         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x27ad(%RIP),%YMM1,%YMM0         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVINSERTF128 $0x1,%XMM12,%YMM5,%YMM10   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x279f(%RIP),%YMM11,%YMM14       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVCVTTPS2DQ %YMM10,%YMM11               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVPACKUSDW %YMM3,%YMM14,%YMM7           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM7,%YMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x27a7(%RIP),%YMM9,%YMM14        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x277f(%RIP),%YMM11,%YMM8        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM8,%YMM0,%YMM15           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM15,%YMM6             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x278c(%RIP),%YMM6,%YMM3         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM3,%YMM14,%YMM7           | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVMOVDQA 0x2860(%RIP),%YMM14            | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPERMQ $-0x28,%YMM7,%YMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQA %YMM9,%YMM13                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPERM2I128 $0,%YMM9,%YMM9,%YMM4        | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVPSHUFB 0x27a6(%RIP),%YMM9,%YMM1       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $-0x7c,%YMM9,%YMM8              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2857(%RIP),%YMM9,%YMM7       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM1,%YMM5               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2768(%RIP),%YMM4,%YMM2       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x27bf(%RIP),%YMM9,%YMM10      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x27f6(%RIP),%YMM9,%YMM6       | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM7,%YMM9               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSHUFB 0x2847(%RIP),%YMM13,%YMM4      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM5,%YMM10,%YMM11               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x285a(%RIP),%YMM13,%YMM1      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x2771(%RIP),%YMM2,%YMM12      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPERMQ $0x4e,%YMM1,%YMM5               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPOR %YMM9,%YMM4,%YMM2                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPOR %YMM11,%YMM12,%YMM0               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2878(%RIP),%YMM13,%YMM13     | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPSHUFB 0x278f(%RIP),%YMM8,%YMM15      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPOR %YMM5,%YMM13,%YMM10               | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSHUFB 0x2842(%RIP),%YMM2,%YMM12      | 13    | 12   | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 23-25   | 13\nVPBLENDVB %YMM14,%YMM6,%YMM15,%YMM3    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 8-10    | 8\nVMOVDQU %YMM0,-0x60(%R11)              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPOR %YMM10,%YMM12,%YMM11              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQU %YMM3,-0x40(%R11)              | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVDQU %YMM11,-0x20(%R11)             | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nJNE 1660 <main._omp_fn.0+0xc0>         | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "1% of peak computational performance is used (0.38 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 38% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 417.00 to 171.00 cycles (2.44x speedup).",
        },
        {
          workaround = " - Reduce the number of FP multiply/FMA instructions\n - Reduce arithmetical operations on array elements\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - execution of FP multiply or FMA (fused multiply-add) operations (the FP multiply/FMA unit is a bottleneck)\n - execution of INT/FP operations in vector registers (the VPU is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 417.00 to 95.00 cycles (4.39x speedup).\n",
        },
      },
      potential = {
        {
          title = "FMA",
          txt = "Detected 64 FMA (fused multiply-add) operations.",
        },
      },
    },
  common = {
    header = {
      "The loop is defined in /scratch/chps/users/user22024/Projet/AOC-Computer-vision/sobel.c:21-30.\n",
      "The related source loop is not unrolled or unrolled with no peel/tail loop.",
    },
    nb_paths = 1,
  },
}
