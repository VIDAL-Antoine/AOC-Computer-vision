_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 8 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 904 bytes.\nThe binary loop is storing 128 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 139\nloop length        : 780\nused x86 registers : 10\nused mmx registers : 0\nused xmm registers : 15\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 6\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 49.00 cycles\ninstruction queue    : 69.50 cycles\ndecoding             : 69.50 cycles\nmicro-operation queue: 70.00 cycles\nfront end            : 70.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n------------------------------------------------------------\nuops   | 116.00 | 52.00 | 19.00 | 19.00 | 1.50 | 1.50 | 4.00\ncycles | 116.00 | 52.00 | 19.00 | 19.00 | 1.50 | 1.50 | 4.00\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 70.00\nDispatch  : 116.00\nData deps.: 1.00\nOverall L1: 116.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 39%\nload    : 44%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 35%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 116.00 cycles. At this rate:\n - 6% of peak load performance is reached (7.79 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 1% of peak store performance is reached (1.10 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1cd5\n\nInstruction                                | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n-------------------------------------------------------------------------------------------------------------------------------\nMOV 0x90(%RSP),%RDX                        | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVMOVDQA (%R11,%RAX,1),%YMM6                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQU (%R13,%RAX,1),%YMM7                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQU (%RDX,%RAX,1),%YMM11               | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW (%R11,%RAX,1),%YMM8              | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R13,%RAX,1),%YMM0              | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM6,%XMM14             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM0,%YMM8,%YMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM7,%XMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM14,%YMM3                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM9,%YMM4                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM11,%YMM8                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM0             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM4,%YMM3,%YMM1                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM15,%YMM13                    | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM1,%YMM5                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM15,%XMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM0,%YMM15                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R12,%RAX,1),%YMM4                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVSXWD %XMM2,%YMM9                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM8,%XMM6              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM8,%YMM14                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM3             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM6,%YMM2                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R12,%RAX,1),%YMM7              | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM15,%YMM6                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM5,0x40(%RSP)                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM7,%YMM0                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R10,%RAX,1),%YMM15             | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM1,%XMM12             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM3,%YMM1                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM4,%XMM5              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM12,%YMM10                    | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVZXBW (%R9,%RAX,1),%YMM3               | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R10,%RAX,1),%YMM4                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVEXTRACTI128 $0x1,%YMM7,%XMM12             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM3,%YMM15,%YMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM11,0xa0(%RSP)                  | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXBW %XMM5,%YMM11                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R9,%RAX,1),%YMM15                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSLLW $0x1,%YMM7,%YMM7                    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQA %YMM10,0x20(%RSP)                  | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM12,%YMM10                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM8             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM11,%YMM12                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM3             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM8,%YMM5                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM4,%XMM11             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM3,%YMM4                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%RSI,%RAX,1),%YMM15             | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM11,%YMM8                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPSUBW %YMM4,%YMM8,%YMM11                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXBW (%R8,%RAX,1),%YMM8               | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPSLLW $0x1,%YMM11,%YMM3                   | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%R8,%RAX,1),%YMM11                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBW %YMM15,%YMM8,%YMM4                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM11,%XMM15            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSLLW $0x1,%YMM4,%YMM8                    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%RSI,%RAX,1),%YMM11               | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW %XMM15,%YMM4                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM15            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM15,%YMM11                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM8,%YMM15                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM8,%XMM8              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM11,%YMM4,%YMM4                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM14,%YMM0,%YMM11                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM0,%YMM14,%YMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM15,%YMM11,%YMM11                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM7,%YMM14                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD %YMM13,%YMM11,%YMM15                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSLLW $0x1,%YMM4,%YMM4                    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVPABSD %YMM15,%YMM11                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM14,%YMM0,%YMM15                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM2,%YMM10,%YMM14                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM10,%YMM2,%YMM2                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM7,%XMM10             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM13,%YMM15,%YMM13                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM10,%YMM7                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM8,%YMM15                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPABSD %YMM13,%YMM0                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM7,%YMM2,%YMM8                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM15,%YMM14,%YMM13                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM0,%YMM11,%YMM11                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x40(%RSP),%YMM2                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPADDD %YMM9,%YMM13,%YMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM0,%YMM14                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM9,%YMM8,%YMM9                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM9,%YMM15                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM6,%YMM12,%YMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM15,%YMM14,%YMM13                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM12,%YMM6,%YMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM4,%YMM14                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM3,%YMM12                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD %YMM14,%YMM0,%YMM10                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM12,%YMM6,%YMM9                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM2,%YMM10,%YMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM2,%YMM9,%YMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM4,%XMM4              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM1,%YMM5,%YMM10                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM5,%YMM1,%YMM1                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM4,%YMM2                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM3,%XMM5              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPCMPGTD 0x1446(%RIP),%YMM11,%YMM4         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM7,%YMM8                         | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM5,%YMM3                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPABSD %YMM15,%YMM0                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM3,%YMM1,%YMM9                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM0,%YMM8,%YMM14                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM2,%YMM10,%YMM8                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x20(%RSP),%YMM7                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPCMPGTD 0x141d(%RIP),%YMM13,%YMM2         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM7,%YMM8,%YMM6                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM7,%YMM9,%YMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM6,%YMM12                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPCMPGTD 0x1408(%RIP),%YMM14,%YMM1         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM15,%YMM0                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPBLENDVB %YMM4,0x1419(%RIP),%YMM11,%YMM11 | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPADDD %YMM0,%YMM12,%YMM10                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPBLENDVB %YMM2,0x140b(%RIP),%YMM13,%YMM13 | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPCMPGTD 0x13e3(%RIP),%YMM10,%YMM5         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPBLENDVB %YMM1,0x13f9(%RIP),%YMM14,%YMM14 | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPBLENDVB %YMM5,0x13ef(%RIP),%YMM10,%YMM3  | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPAND 0x1207(%RIP),%YMM11,%YMM8            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x11ff(%RIP),%YMM13,%YMM7            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x11f7(%RIP),%YMM14,%YMM9            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x11ef(%RIP),%YMM3,%YMM15            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM7,%YMM8,%YMM6                | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPACKUSDW %YMM15,%YMM9,%YMM0               | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM6,%YMM12                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPERMQ $-0x28,%YMM0,%YMM10                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x11f1(%RIP),%YMM12,%YMM4            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nMOV 0x98(%RSP),%RDX                        | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVPAND 0x11e1(%RIP),%YMM10,%YMM11           | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM11,%YMM4,%YMM2               | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM2,%YMM13                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQU %YMM13,(%RDX,%RAX,1)               | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nLEA 0x20(%RAX),%RAX                        | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nCMP %RAX,0x88(%RSP)                        | 1     | 0    | 0    | 1    | 0    | 0.50 | 0.50 | 0  | 1       | 1\nJNE 1cd5 <main._omp_fn.0+0x775>            | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 39% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 116.00 to 41.50 cycles (2.80x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 116.00 to 70.00 cycles (1.66x speedup).\n",
        },
      },
      potential = {
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 8 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 904 bytes.\nThe binary loop is storing 128 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 139\nloop length        : 780\nused x86 registers : 10\nused mmx registers : 0\nused xmm registers : 15\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 6\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 49.00 cycles\ninstruction queue    : 69.50 cycles\ndecoding             : 69.50 cycles\nmicro-operation queue: 70.00 cycles\nfront end            : 70.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0     | P1    | P2    | P3    | P4   | P5   | P6\n------------------------------------------------------------\nuops   | 116.00 | 52.00 | 19.00 | 19.00 | 1.50 | 1.50 | 4.00\ncycles | 116.00 | 52.00 | 19.00 | 19.00 | 1.50 | 1.50 | 4.00\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 70.00\nDispatch  : 116.00\nData deps.: 1.00\nOverall L1: 116.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 39%\nload    : 44%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 35%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 116.00 cycles. At this rate:\n - 6% of peak load performance is reached (7.79 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 1% of peak store performance is reached (1.10 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1cd5\n\nInstruction                                | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n-------------------------------------------------------------------------------------------------------------------------------\nMOV 0x90(%RSP),%RDX                        | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVMOVDQA (%R11,%RAX,1),%YMM6                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQU (%R13,%RAX,1),%YMM7                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVDQU (%RDX,%RAX,1),%YMM11               | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW (%R11,%RAX,1),%YMM8              | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R13,%RAX,1),%YMM0              | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM6,%XMM14             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM0,%YMM8,%YMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM7,%XMM9              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM14,%YMM3                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM9,%YMM4                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM11,%YMM8                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM0             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM4,%YMM3,%YMM1                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM15,%YMM13                    | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM1,%YMM5                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM15,%XMM2             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM0,%YMM15                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R12,%RAX,1),%YMM4                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVSXWD %XMM2,%YMM9                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM8,%XMM6              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM8,%YMM14                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM3             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM6,%YMM2                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R12,%RAX,1),%YMM7              | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXWD %XMM15,%YMM6                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQA %YMM5,0x40(%RSP)                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM7,%YMM0                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%R10,%RAX,1),%YMM15             | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM1,%XMM12             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM3,%YMM1                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM4,%XMM5              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVSXWD %XMM12,%YMM10                    | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVZXBW (%R9,%RAX,1),%YMM3               | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R10,%RAX,1),%YMM4                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVEXTRACTI128 $0x1,%YMM7,%XMM12             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM3,%YMM15,%YMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA %YMM11,0xa0(%RSP)                  | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXBW %XMM5,%YMM11                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVMOVDQU (%R9,%RAX,1),%YMM15                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSLLW $0x1,%YMM7,%YMM7                    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQA %YMM10,0x20(%RSP)                  | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVPMOVZXWD %XMM12,%YMM10                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM8             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM11,%YMM12                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM15,%XMM3             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXWD %XMM8,%YMM5                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM4,%XMM11             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM3,%YMM4                      | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW (%RSI,%RAX,1),%YMM15             | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPMOVZXBW %XMM11,%YMM8                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPSUBW %YMM4,%YMM8,%YMM11                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVZXBW (%R8,%RAX,1),%YMM8               | 1     | 1    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 3       | 1\nVPSLLW $0x1,%YMM11,%YMM3                   | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%R8,%RAX,1),%YMM11                | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPSUBW %YMM15,%YMM8,%YMM4                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM11,%XMM15            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSLLW $0x1,%YMM4,%YMM8                    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVMOVDQU (%RSI,%RAX,1),%YMM11               | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPMOVZXBW %XMM15,%YMM4                     | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVEXTRACTI128 $0x1,%YMM11,%XMM15            | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPMOVZXBW %XMM15,%YMM11                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3       | 1\nVPMOVSXWD %XMM8,%YMM15                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM8,%XMM8              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBW %YMM11,%YMM4,%YMM4                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM14,%YMM0,%YMM11                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM0,%YMM14,%YMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM15,%YMM11,%YMM11                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM7,%YMM14                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD %YMM13,%YMM11,%YMM15                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSLLW $0x1,%YMM4,%YMM4                    | 4     | 4    | 0    | 0    | 0    | 0    | 0    | 0  | 11      | 8\nVPABSD %YMM15,%YMM11                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM14,%YMM0,%YMM15                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM2,%YMM10,%YMM14                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM10,%YMM2,%YMM2                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM7,%XMM10             | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPADDD %YMM13,%YMM15,%YMM13                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM10,%YMM7                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM8,%YMM15                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPABSD %YMM13,%YMM0                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM7,%YMM2,%YMM8                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM15,%YMM14,%YMM13                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM0,%YMM11,%YMM11                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x40(%RSP),%YMM2                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPADDD %YMM9,%YMM13,%YMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM0,%YMM14                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM9,%YMM8,%YMM9                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM9,%YMM15                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM6,%YMM12,%YMM0                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM15,%YMM14,%YMM13                | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM12,%YMM6,%YMM6                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM4,%YMM14                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPMOVSXWD %XMM3,%YMM12                     | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPADDD %YMM14,%YMM0,%YMM10                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM12,%YMM6,%YMM9                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM2,%YMM10,%YMM7                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM2,%YMM9,%YMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVEXTRACTI128 $0x1,%YMM4,%XMM4              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPSUBD %YMM1,%YMM5,%YMM10                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPSUBD %YMM5,%YMM1,%YMM1                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM4,%YMM2                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVEXTRACTI128 $0x1,%YMM3,%XMM5              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPCMPGTD 0x1446(%RIP),%YMM11,%YMM4         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM7,%YMM8                         | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPMOVSXWD %XMM5,%YMM3                      | 2     | 2    | 0    | 0    | 0    | 0    | 0    | 0  | 8       | 7-8\nVPABSD %YMM15,%YMM0                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM3,%YMM1,%YMM9                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM0,%YMM8,%YMM14                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM2,%YMM10,%YMM8                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVDQA 0x20(%RSP),%YMM7                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVPCMPGTD 0x141d(%RIP),%YMM13,%YMM2         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM7,%YMM8,%YMM6                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPADDD %YMM7,%YMM9,%YMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM6,%YMM12                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPCMPGTD 0x1408(%RIP),%YMM14,%YMM1         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPABSD %YMM15,%YMM0                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPBLENDVB %YMM4,0x1419(%RIP),%YMM11,%YMM11 | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPADDD %YMM0,%YMM12,%YMM10                 | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVPBLENDVB %YMM2,0x140b(%RIP),%YMM13,%YMM13 | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPCMPGTD 0x13e3(%RIP),%YMM10,%YMM5         | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPBLENDVB %YMM1,0x13f9(%RIP),%YMM14,%YMM14 | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPBLENDVB %YMM5,0x13ef(%RIP),%YMM10,%YMM3  | 5     | 4    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 8-10    | 8\nVPAND 0x1207(%RIP),%YMM11,%YMM8            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x11ff(%RIP),%YMM13,%YMM7            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x11f7(%RIP),%YMM14,%YMM9            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPAND 0x11ef(%RIP),%YMM3,%YMM15            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSDW %YMM7,%YMM8,%YMM6                | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPACKUSDW %YMM15,%YMM9,%YMM0               | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM6,%YMM12                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPERMQ $-0x28,%YMM0,%YMM10                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVPAND 0x11f1(%RIP),%YMM12,%YMM4            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nMOV 0x98(%RSP),%RDX                        | 1     | 0    | 0    | 1    | 0    | 0    | 0    | 0  | 4       | 1\nVPAND 0x11e1(%RIP),%YMM10,%YMM11           | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 2       | 0.50\nVPACKUSWB %YMM11,%YMM4,%YMM2               | 5     | 5    | 0    | 0    | 0    | 0    | 0    | 0  | 11-14   | 9\nVPERMQ $-0x28,%YMM2,%YMM13                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nVMOVDQU %YMM13,(%RDX,%RAX,1)               | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nLEA 0x20(%RAX),%RAX                        | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nCMP %RAX,0x88(%RSP)                        | 1     | 0    | 0    | 1    | 0    | 0.50 | 0.50 | 0  | 1       | 1\nJNE 1cd5 <main._omp_fn.0+0x775>            | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 4.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 39% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 116.00 to 41.50 cycles (2.80x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 116.00 to 70.00 cycles (1.66x speedup).\n",
        },
      },
      potential = {
      },
    },
  common = {
    header = {
      "The loop is defined in /scratch/chps/users/user22024/Projet/AOC-Computer-vision/sobel.c:204-214.\n",
      "It is main loop of related source loop which is unrolled by 32 (including vectorization).",
    },
    nb_paths = 1,
  },
}
