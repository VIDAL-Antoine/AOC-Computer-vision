_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant non-unit stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 304 bytes.\nThe binary loop is storing 80 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 138\nnb uops            : 137\nloop length        : 720\nused x86 registers : 11\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 0\nused zmm registers : 0\nnb stack references: 4\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 22.83 cycles\nfront end            : 22.83 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | ALU0 | ALU1 | ALU2 | ALU3 | AGU0  | AGU1  | FP0   | FP1   | FP2   | FP3\n----------------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.50 | 0.50 | 12.00 | 12.00 | 28.00 | 33.00 | 33.00 | 28.00\ncycles | 0.50 | 0.50 | 0.50 | 0.50 | 12.00 | 12.00 | 28.00 | 33.00 | 33.00 | 28.00\n\nCycles executing div or sqrt instructions: NA\nCycles loading/storing data              : 9.50\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 22.83\nDispatch  : 33.00\nData deps.: 1.00\nOverall L1: 33.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 43%\nload    : 50%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 39%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 33.00 cycles. At this rate:\n - 28% of peak load performance is reached (9.21 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 15% of peak store performance is reached (2.42 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1f72\n\nInstruction                          | Nb FU | ALU0 | ALU1 | ALU2 | ALU3 | AGU0 | AGU1 | FP0  | FP1  | FP2  | FP3  | Latency | Recip. throughput\n------------------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%R13,%RDX,1),%XMM12         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQA (%RCX,%RDX,1),%XMM7          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM12,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM7,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM12,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM7,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM2,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM5,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM0,%XMM15,%XMM10           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R15,%RDX,1),%XMM15         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSUBW %XMM6,%XMM14,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RBX,%RDX,1),%XMM6          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVSXWD %XMM10,%XMM8               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM4            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM11,%XMM1               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM12           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQA %XMM8,0x20(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVSXWD %XMM4,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM1,0x40(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVSXWD %XMM12,%XMM7               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM3,0x30(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM7,0x50(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVZXBW %XMM15,%XMM10              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM15,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM0,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM6,%XMM11               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM10,%XMM0               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM14,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%RBP,%RDX,1),%XMM10         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM2,%XMM8                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM1            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM14,%XMM2               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM11,%XMM14              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R9,%RDX,1),%XMM11          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM5,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM6,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM4,%XMM7                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM1,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM7,%XMM12               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM7,%XMM15            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM15,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM10,%XMM6               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM7            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM7,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R8,%RDX,1),%XMM7           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM11,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM5,%XMM6,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVZXBW %XMM11,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSLLW $0x1,%XMM4,%XMM6              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM5,%XMM10,%XMM4            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RDI,%RDX,1),%XMM10         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM4,%XMM4              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM7,%XMM11               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM10,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM10           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM5,%XMM11,%XMM7            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R8,%RDX,1),%XMM11          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM7,%XMM7              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM11,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM10,%XMM11              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM5,%XMM5                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM7,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM11,%XMM5,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM0,%XMM14,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM14,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM6,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM10,%XMM11,%XMM11          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x20(%RSP),%XMM10            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM14,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM7,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSLLW $0x1,%XMM5,%XMM5              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM10,%XMM0,%XMM14           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM10,%XMM11,%XMM11          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM14,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPMOVSXWD %XMM7,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM0,%XMM11,%XMM10           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM8,%XMM1,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM1,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM6,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM14,%XMM11,%XMM0           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x30(%RSP),%XMM11            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVSXWD %XMM1,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM5,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM5,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM11,%XMM0,%XMM7            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM6,%XMM8,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM2,%XMM12,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM11,%XMM0,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x40(%RSP),%XMM0             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSUBD %XMM12,%XMM2,%XMM12           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM4,%XMM2                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM1,%XMM8,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM2,%XMM12,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM7,%XMM14                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM7,%XMM14,%XMM14           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM0,%XMM8,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x50(%RSP),%XMM8             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM0,%XMM6,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM3,%XMM15,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM1,%XMM6                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPSUBD %XMM15,%XMM3,%XMM15           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM4,%XMM3             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM6,%XMM7,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM3,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM5,%XMM7                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM7,%XMM0,%XMM12            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM4,%XMM15,%XMM6            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM8,%XMM12,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM8,%XMM6,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD %XMM13,%XMM10,%XMM12        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM2,%XMM1                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM0,%XMM5                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD %XMM13,%XMM14,%XMM8         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD %XMM13,%XMM11,%XMM2         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM5,%XMM1,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPBLENDVB %XMM12,%XMM9,%XMM10,%XMM10 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPBLENDVB %XMM8,%XMM9,%XMM14,%XMM14  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2f4c(%RIP),%XMM10,%XMM4      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPCMPGTD %XMM13,%XMM7,%XMM1          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPAND 0x2f3f(%RIP),%XMM14,%XMM3      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPBLENDVB %XMM2,%XMM9,%XMM11,%XMM11  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2f31(%RIP),%XMM11,%XMM0      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPBLENDVB %XMM1,%XMM9,%XMM7,%XMM15   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2f23(%RIP),%XMM15,%XMM5      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM3,%XMM4,%XMM6          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2f26(%RIP),%XMM6,%XMM12      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM5,%XMM0,%XMM7          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2f19(%RIP),%XMM7,%XMM10      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSWB %XMM10,%XMM12,%XMM8        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM8,(%RSI,%RDX,1)          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nADD $0x10,%RDX                       | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nCMP $0xef0,%RDX                      | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nJNE 1f72 <main+0xe12>                | 1     | 0.50 | 0    | 0    | 0.50 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.50\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 6.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 32 bytes boundaries\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 32 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 32) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 43% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 33.00 to 13.50 cycles (2.44x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 33.00 to 28.00 cycles (1.18x speedup).\n",
        },
      },
      potential = {
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant non-unit stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 304 bytes.\nThe binary loop is storing 80 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 138\nnb uops            : 137\nloop length        : 720\nused x86 registers : 11\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 0\nused zmm registers : 0\nnb stack references: 4\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 22.83 cycles\nfront end            : 22.83 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | ALU0 | ALU1 | ALU2 | ALU3 | AGU0  | AGU1  | FP0   | FP1   | FP2   | FP3\n----------------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.50 | 0.50 | 12.00 | 12.00 | 28.00 | 33.00 | 33.00 | 28.00\ncycles | 0.50 | 0.50 | 0.50 | 0.50 | 12.00 | 12.00 | 28.00 | 33.00 | 33.00 | 28.00\n\nCycles executing div or sqrt instructions: NA\nCycles loading/storing data              : 9.50\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 22.83\nDispatch  : 33.00\nData deps.: 1.00\nOverall L1: 33.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 43%\nload    : 50%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 39%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 33.00 cycles. At this rate:\n - 28% of peak load performance is reached (9.21 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 15% of peak store performance is reached (2.42 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1f72\n\nInstruction                          | Nb FU | ALU0 | ALU1 | ALU2 | ALU3 | AGU0 | AGU1 | FP0  | FP1  | FP2  | FP3  | Latency | Recip. throughput\n------------------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%R13,%RDX,1),%XMM12         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQA (%RCX,%RDX,1),%XMM7          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM12,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM7,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM12,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM7,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM2,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM5,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM0,%XMM15,%XMM10           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R15,%RDX,1),%XMM15         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSUBW %XMM6,%XMM14,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RBX,%RDX,1),%XMM6          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVSXWD %XMM10,%XMM8               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM4            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM11,%XMM1               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM12           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQA %XMM8,0x20(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVSXWD %XMM4,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM1,0x40(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVSXWD %XMM12,%XMM7               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM3,0x30(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM7,0x50(%RSP)             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVZXBW %XMM15,%XMM10              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM15,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM0,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM6,%XMM11               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM10,%XMM0               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM14,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%RBP,%RDX,1),%XMM10         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM2,%XMM8                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM1            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM14,%XMM2               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM11,%XMM14              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R9,%RDX,1),%XMM11          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM5,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM6,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM4,%XMM7                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM1,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM7,%XMM12               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM7,%XMM15            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM15,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM10,%XMM6               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM7            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM7,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R8,%RDX,1),%XMM7           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM11,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM5,%XMM6,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVZXBW %XMM11,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSLLW $0x1,%XMM4,%XMM6              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM5,%XMM10,%XMM4            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RDI,%RDX,1),%XMM10         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM4,%XMM4              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM7,%XMM11               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM10,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM10           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM5,%XMM11,%XMM7            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R8,%RDX,1),%XMM11          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM7,%XMM7              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM11,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM10,%XMM11              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM5,%XMM5                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM7,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM11,%XMM5,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM0,%XMM14,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM14,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM6,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM10,%XMM11,%XMM11          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x20(%RSP),%XMM10            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM14,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM7,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSLLW $0x1,%XMM5,%XMM5              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM10,%XMM0,%XMM14           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM10,%XMM11,%XMM11          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM14,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPMOVSXWD %XMM7,%XMM14               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM0,%XMM11,%XMM10           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM8,%XMM1,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM1,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM6,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM14,%XMM11,%XMM0           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x30(%RSP),%XMM11            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVSXWD %XMM1,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM5,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM5,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM11,%XMM0,%XMM7            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM6,%XMM8,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM2,%XMM12,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM11,%XMM0,%XMM11           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x40(%RSP),%XMM0             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSUBD %XMM12,%XMM2,%XMM12           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM4,%XMM2                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM1,%XMM8,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM2,%XMM12,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM7,%XMM14                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM7,%XMM14,%XMM14           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM0,%XMM8,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA 0x50(%RSP),%XMM8             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM0,%XMM6,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM3,%XMM15,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM1,%XMM6                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPSUBD %XMM15,%XMM3,%XMM15           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM4,%XMM3             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM6,%XMM7,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM3,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM5,%XMM7                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM7,%XMM0,%XMM12            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM4,%XMM15,%XMM6            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM8,%XMM12,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM8,%XMM6,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD %XMM13,%XMM10,%XMM12        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM2,%XMM1                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM0,%XMM5                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD %XMM13,%XMM14,%XMM8         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD %XMM13,%XMM11,%XMM2         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM5,%XMM1,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPBLENDVB %XMM12,%XMM9,%XMM10,%XMM10 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPBLENDVB %XMM8,%XMM9,%XMM14,%XMM14  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2f4c(%RIP),%XMM10,%XMM4      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPCMPGTD %XMM13,%XMM7,%XMM1          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPAND 0x2f3f(%RIP),%XMM14,%XMM3      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPBLENDVB %XMM2,%XMM9,%XMM11,%XMM11  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2f31(%RIP),%XMM11,%XMM0      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPBLENDVB %XMM1,%XMM9,%XMM7,%XMM15   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2f23(%RIP),%XMM15,%XMM5      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM3,%XMM4,%XMM6          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2f26(%RIP),%XMM6,%XMM12      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM5,%XMM0,%XMM7          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2f19(%RIP),%XMM7,%XMM10      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSWB %XMM10,%XMM12,%XMM8        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM8,(%RSI,%RDX,1)          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nADD $0x10,%RDX                       | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nCMP $0xef0,%RDX                      | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nJNE 1f72 <main+0xe12>                | 1     | 0.50 | 0    | 0    | 0.50 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.50\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 6.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 32 bytes boundaries\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 32 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 32) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 43% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 33.00 to 13.50 cycles (2.44x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 33.00 to 28.00 cycles (1.18x speedup).\n",
        },
      },
      potential = {
      },
    },
  common = {
    header = {
      "The loop is defined in /home/vidal/Desktop/AOC_oseret/Projet/AOC-Computer-vision/sobel.c:166-176.\n",
      "It is main loop of related source loop which is unrolled by 4 (including vectorization).",
    },
    nb_paths = 1,
  },
}
