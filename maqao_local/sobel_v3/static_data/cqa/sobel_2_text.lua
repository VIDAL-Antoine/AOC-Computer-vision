_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant non-unit stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 400 bytes.\nThe binary loop is storing 48 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 133\nnb uops            : 132\nloop length        : 728\nused x86 registers : 11\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 0\nused zmm registers : 0\nnb stack references: 2\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 22.00 cycles\nfront end            : 22.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | ALU0 | ALU1 | ALU2 | ALU3 | AGU0  | AGU1  | FP0   | FP1   | FP2   | FP3\n----------------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.50 | 0.50 | 14.00 | 14.00 | 28.00 | 32.00 | 32.00 | 28.00\ncycles | 0.50 | 0.50 | 0.50 | 0.50 | 14.00 | 14.00 | 28.00 | 32.00 | 32.00 | 28.00\n\nCycles executing div or sqrt instructions: NA\nCycles loading/storing data              : 12.50\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 22.00\nDispatch  : 32.00\nData deps.: 1.00\nOverall L1: 32.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 43%\nload    : 50%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 39%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 32.00 cycles. At this rate:\n - 39% of peak load performance is reached (12.50 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 9% of peak store performance is reached (1.50 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 2335\n\nInstruction                                | Nb FU | ALU0 | ALU1 | ALU2 | ALU3 | AGU0 | AGU1 | FP0  | FP1  | FP2  | FP3  | Latency | Recip. throughput\n------------------------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%R12,%RDX,1),%XMM10               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQA (%RCX,%RDX,1),%XMM4                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQU (%R13,%RDX,1),%XMM3                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM11,%XMM13                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R11,%RDX,1),%XMM11               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM10,%XMM0                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM4,%XMM14                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM4,%XMM6                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM6,%XMM5                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM3,%XMM2                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM14,%XMM0,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBW %XMM5,%XMM13,%XMM15                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM3,%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM10,%XMM14                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM7,%XMM12                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM12,%XMM9                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM15,%XMM1                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM15,%XMM8                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM8,%XMM15                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM7,%XMM13                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM2,%XMM0                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM14,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQA %XMM1,0x30(%RSP)                   | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPSRLDQ $0x8,%XMM2,%XMM4                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM7,%XMM1                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM14,%XMM6                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM4,%XMM4                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM11,%XMM5                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM12                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R10,%RDX,1),%XMM11               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM12,%XMM8                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM5,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM3,%XMM10                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R14,%RDX,1),%XMM3                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM8,%XMM12                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM8,%XMM2                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM5,%XMM14                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM2,%XMM5                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM11,%XMM7                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM3,%XMM8                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM8,%XMM7,%XMM2                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVZXBW %XMM11,%XMM8                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSLLW $0x1,%XMM2,%XMM7                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM3,%XMM2                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM2,%XMM8,%XMM11                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R15,%RDX,1),%XMM8                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM11,%XMM2                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%RDI,%RDX,1),%XMM11               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM8,%XMM3                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM11,%XMM8                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM11,%XMM11                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSLLW $0x1,%XMM3,%XMM8                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R15,%RDX,1),%XMM3                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSRLDQ $0x8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM3,%XMM3                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM11,%XMM3,%XMM3                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM0,%XMM14,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM14,%XMM0,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM7,%XMM14                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM11,0x40(%RSP)                  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVSXWD %XMM8,%XMM11                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD 0x40(%RSP),%XMM11,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.50\nVPADDD %XMM14,%XMM0,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM8,%XMM8                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSLLW $0x1,%XMM3,%XMM3                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM13,%XMM11,%XMM11                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM13,%XMM0,%XMM13                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM13,%XMM14                       | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM11                       | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPMOVSXWD %XMM8,%XMM13                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM14,%XMM11,%XMM0                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM4,%XMM10,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM10,%XMM4,%XMM4                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM7,%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM10,%XMM7                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA 0x30(%RSP),%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM13,%XMM11,%XMM14                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM7,%XMM4,%XMM13                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM9,%XMM14,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM9,%XMM13,%XMM9                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM11,%XMM8                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPSUBD %XMM6,%XMM12,%XMM4                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM9,%XMM14                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPSUBD %XMM12,%XMM6,%XMM6                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM2,%XMM12                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM14,%XMM8,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM3,%XMM8                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM12,%XMM6,%XMM14                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM8,%XMM4,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM10,%XMM7,%XMM13                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM10,%XMM14,%XMM4                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM1,%XMM5,%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM13,%XMM9                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM4,%XMM8                         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPMOVSXWD %XMM3,%XMM13                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM5,%XMM1,%XMM1                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM2,%XMM5                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM5,%XMM2                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM8,%XMM9,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM13,%XMM10,%XMM9                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM2,%XMM1,%XMM14                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM15,%XMM9,%XMM6                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD 0x2beb(%RIP),%XMM0,%XMM10         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM15,%XMM14,%XMM15                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM6,%XMM12                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD 0x2bd9(%RIP),%XMM11,%XMM3         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM10,0x2bdf(%RIP),%XMM0,%XMM0  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPABSD %XMM15,%XMM4                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD 0x2bc2(%RIP),%XMM7,%XMM13         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM3,0x2bc8(%RIP),%XMM11,%XMM11 | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2b90(%RIP),%XMM11,%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPADDD %XMM4,%XMM12,%XMM8                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPBLENDVB %XMM13,0x2bb2(%RIP),%XMM7,%XMM7  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2b7a(%RIP),%XMM0,%XMM12            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPAND 0x2b72(%RIP),%XMM7,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPCMPGTD 0x2b8a(%RIP),%XMM8,%XMM9          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM9,0x2b90(%RIP),%XMM8,%XMM6   | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2b58(%RIP),%XMM6,%XMM14            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM1,%XMM12,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2b5b(%RIP),%XMM5,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM14,%XMM2,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2b4e(%RIP),%XMM15,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSWB %XMM8,%XMM4,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM10,(%RSI,%RDX,1)               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nADD $0x10,%RDX                             | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nCMP $0xef0,%RDX                            | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nJNE 2335 <main._omp_fn.0+0xdf5>            | 1     | 0.50 | 0    | 0    | 0.50 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.50\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 6.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 32 bytes boundaries\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 32 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 32) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 43% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 32.00 to 12.83 cycles (2.49x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 32.00 to 28.00 cycles (1.14x speedup).\n",
        },
      },
      potential = {
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant non-unit stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 400 bytes.\nThe binary loop is storing 48 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 133\nnb uops            : 132\nloop length        : 728\nused x86 registers : 11\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 0\nused zmm registers : 0\nnb stack references: 2\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 22.00 cycles\nfront end            : 22.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | ALU0 | ALU1 | ALU2 | ALU3 | AGU0  | AGU1  | FP0   | FP1   | FP2   | FP3\n----------------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.50 | 0.50 | 14.00 | 14.00 | 28.00 | 32.00 | 32.00 | 28.00\ncycles | 0.50 | 0.50 | 0.50 | 0.50 | 14.00 | 14.00 | 28.00 | 32.00 | 32.00 | 28.00\n\nCycles executing div or sqrt instructions: NA\nCycles loading/storing data              : 12.50\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 22.00\nDispatch  : 32.00\nData deps.: 1.00\nOverall L1: 32.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 43%\nload    : 50%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 39%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 32.00 cycles. At this rate:\n - 39% of peak load performance is reached (12.50 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 9% of peak store performance is reached (1.50 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 2335\n\nInstruction                                | Nb FU | ALU0 | ALU1 | ALU2 | ALU3 | AGU0 | AGU1 | FP0  | FP1  | FP2  | FP3  | Latency | Recip. throughput\n------------------------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%R12,%RDX,1),%XMM10               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQA (%RCX,%RDX,1),%XMM4                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQU (%R13,%RDX,1),%XMM3                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM11,%XMM13                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R11,%RDX,1),%XMM11               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM10,%XMM0                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM4,%XMM14                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM4,%XMM6                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM6,%XMM5                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM3,%XMM2                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM14,%XMM0,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBW %XMM5,%XMM13,%XMM15                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM3,%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM10,%XMM14                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM7,%XMM12                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM12,%XMM9                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM15,%XMM1                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM15,%XMM8                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM8,%XMM15                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM7,%XMM13                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM2,%XMM0                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM14,%XMM7                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQA %XMM1,0x30(%RSP)                   | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPSRLDQ $0x8,%XMM2,%XMM4                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM7,%XMM1                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM14,%XMM6                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM4,%XMM4                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM11,%XMM5                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM12                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R10,%RDX,1),%XMM11               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM12,%XMM8                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM5,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM3,%XMM10                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R14,%RDX,1),%XMM3                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM8,%XMM12                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM8,%XMM2                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM5,%XMM14                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM2,%XMM5                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM11,%XMM7                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM3,%XMM8                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM8,%XMM7,%XMM2                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVZXBW %XMM11,%XMM8                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSLLW $0x1,%XMM2,%XMM7                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM3,%XMM2                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM2,%XMM8,%XMM11                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R15,%RDX,1),%XMM8                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM11,%XMM2                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%RDI,%RDX,1),%XMM11               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM8,%XMM3                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM11,%XMM8                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM11,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM11,%XMM11                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSLLW $0x1,%XMM3,%XMM8                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R15,%RDX,1),%XMM3                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSRLDQ $0x8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM3,%XMM3                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM11,%XMM3,%XMM3                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM0,%XMM14,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM14,%XMM0,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM7,%XMM14                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM11,0x40(%RSP)                  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVSXWD %XMM8,%XMM11                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD 0x40(%RSP),%XMM11,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.50\nVPADDD %XMM14,%XMM0,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM8,%XMM8                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSLLW $0x1,%XMM3,%XMM3                    | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM13,%XMM11,%XMM11                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM13,%XMM0,%XMM13                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM13,%XMM14                       | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM11                       | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPMOVSXWD %XMM8,%XMM13                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM14,%XMM11,%XMM0                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM4,%XMM10,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM10,%XMM4,%XMM4                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM7,%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM10,%XMM7                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA 0x30(%RSP),%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM13,%XMM11,%XMM14                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM7,%XMM4,%XMM13                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM9,%XMM14,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM9,%XMM13,%XMM9                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM11,%XMM8                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPSUBD %XMM6,%XMM12,%XMM4                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM9,%XMM14                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPSUBD %XMM12,%XMM6,%XMM6                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM2,%XMM12                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM14,%XMM8,%XMM11                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM3,%XMM8                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM12,%XMM6,%XMM14                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM3,%XMM3                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM8,%XMM4,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM10,%XMM7,%XMM13                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM10,%XMM14,%XMM4                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM1,%XMM5,%XMM10                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM13,%XMM9                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM4,%XMM8                         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPMOVSXWD %XMM3,%XMM13                     | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM5,%XMM1,%XMM1                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM2,%XMM5                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM5,%XMM2                      | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM8,%XMM9,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM13,%XMM10,%XMM9                 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM2,%XMM1,%XMM14                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM15,%XMM9,%XMM6                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD 0x2beb(%RIP),%XMM0,%XMM10         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM15,%XMM14,%XMM15                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM6,%XMM12                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD 0x2bd9(%RIP),%XMM11,%XMM3         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM10,0x2bdf(%RIP),%XMM0,%XMM0  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPABSD %XMM15,%XMM4                        | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD 0x2bc2(%RIP),%XMM7,%XMM13         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM3,0x2bc8(%RIP),%XMM11,%XMM11 | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2b90(%RIP),%XMM11,%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPADDD %XMM4,%XMM12,%XMM8                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPBLENDVB %XMM13,0x2bb2(%RIP),%XMM7,%XMM7  | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2b7a(%RIP),%XMM0,%XMM12            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPAND 0x2b72(%RIP),%XMM7,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPCMPGTD 0x2b8a(%RIP),%XMM8,%XMM9          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM9,0x2b90(%RIP),%XMM8,%XMM6   | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 1    | 0    | 0    | 0    | 1       | 1\nVPAND 0x2b58(%RIP),%XMM6,%XMM14            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM1,%XMM12,%XMM5               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2b5b(%RIP),%XMM5,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSDW %XMM14,%XMM2,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2b4e(%RIP),%XMM15,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSWB %XMM8,%XMM4,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM10,(%RSI,%RDX,1)               | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nADD $0x10,%RDX                             | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nCMP $0xef0,%RDX                            | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nJNE 2335 <main._omp_fn.0+0xdf5>            | 1     | 0.50 | 0    | 0    | 0.50 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.50\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 6.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 32 bytes boundaries\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 32 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 32) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 43% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 32.00 to 12.83 cycles (2.49x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 32.00 to 28.00 cycles (1.14x speedup).\n",
        },
      },
      potential = {
      },
    },
  common = {
    header = {
      "The loop is defined in /home/vidal/Desktop/AOC_oseret/Projet/AOC-Computer-vision/sobel.c:207-217.\n",
      "It is main loop of related source loop which is unrolled by 4 (including vectorization).",
    },
    nb_paths = 1,
  },
}
