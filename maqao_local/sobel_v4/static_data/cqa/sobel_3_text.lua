_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant non-unit stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 256 bytes.\nThe binary loop is storing 96 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 140\nnb uops            : 139\nloop length        : 707\nused x86 registers : 11\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 0\nused zmm registers : 0\nnb stack references: 5\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 23.17 cycles\nfront end            : 23.17 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | ALU0 | ALU1 | ALU2 | ALU3 | AGU0  | AGU1  | FP0   | FP1   | FP2   | FP3\n----------------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.50 | 0.50 | 11.00 | 11.00 | 28.00 | 33.50 | 33.50 | 28.00\ncycles | 0.50 | 0.50 | 0.50 | 0.50 | 11.00 | 11.00 | 28.00 | 33.50 | 33.50 | 28.00\n\nCycles executing div or sqrt instructions: NA\nCycles loading/storing data              : 8.00\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 23.17\nDispatch  : 33.50\nData deps.: 1.00\nOverall L1: 33.50\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 43%\nload    : 50%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 39%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 33.50 cycles. At this rate:\n - 23% of peak load performance is reached (7.64 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 17% of peak store performance is reached (2.87 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 2510\n\nInstruction                          | Nb FU | ALU0 | ALU1 | ALU2 | ALU3 | AGU0 | AGU1 | FP0  | FP1  | FP2  | FP3  | Latency | Recip. throughput\n------------------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%RBP,%RDX,1),%XMM0          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQU (%R13,%RDX,1),%XMM2          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM0,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM2,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM0,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM2,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM3,%XMM1,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R11,%RDX,1),%XMM3          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM5,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM7,%XMM8                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM8,%XMM6,%XMM9             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM4,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM11,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM4,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM9,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM9,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R10,%RDX,1),%XMM9          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVSXWD %XMM2,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM15,-0x58(%RSP)           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM10,-0x68(%RSP)           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM0,-0x48(%RSP)            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM1,-0x38(%RSP)            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPSRLDQ $0x8,%XMM3,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM5,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM3,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM6,%XMM2                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM6,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R8,%RDX,1),%XMM6           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM8,%XMM15               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R9,%RDX,1),%XMM8           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM9,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM9,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM11,%XMM3               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM4,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM3,%XMM9                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM1            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM3,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM4,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM1,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM10,%XMM11              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM5,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM4,-0x78(%RSP)            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVZXWD %XMM7,%XMM7                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM6,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM6,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM8,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM3,%XMM1,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVZXBW %XMM8,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM6,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R14,%RDX,1),%XMM8          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM4,%XMM5              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM3,%XMM1,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RDI,%RDX,1),%XMM1          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM4,%XMM3              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM8,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM1,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM4,%XMM6,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RDI,%RDX,1),%XMM4          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM1,%XMM6              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM4,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM1,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM8,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM0,%XMM11,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBW %XMM1,%XMM4,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM6,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM11,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM5,%XMM11               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM1,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x68(%RSP),%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM11,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM6,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM5,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSLLW $0x1,%XMM4,%XMM4              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM1,%XMM0,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x78(%RSP),%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPABSD %XMM8,%XMM8                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM0,%XMM8,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM6,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA -0x58(%RSP),%XMM6            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSUBD %XMM7,%XMM1,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM1,%XMM7,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM5,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM2,%XMM9,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM0,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM9,%XMM2,%XMM9             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM3,%XMM2                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM6,%XMM8,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM0,%XMM8                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM1,%XMM7,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM4,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM4,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM6,%XMM0,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM5,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM2,%XMM9,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM4,%XMM9                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPABSD %XMM6,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM7,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x48(%RSP),%XMM7            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM7,%XMM0,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM7,%XMM1,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x38(%RSP),%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPABSD %XMM0,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM6,%XMM5                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM7,%XMM5,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM15,%XMM10,%XMM5           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM10,%XMM15,%XMM15          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM3,%XMM10            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM10,%XMM3               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM9,%XMM5,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM3,%XMM15,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM2,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM5,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD %XMM14,%XMM11,%XMM1         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM0,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM4,%XMM9                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD %XMM14,%XMM8,%XMM0          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM1,%XMM12,%XMM11,%XMM11 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPADDD %XMM9,%XMM7,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD %XMM14,%XMM6,%XMM7          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM0,%XMM12,%XMM8,%XMM8   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND %XMM11,%XMM13,%XMM3            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPCMPGTD %XMM14,%XMM2,%XMM15         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM7,%XMM12,%XMM6,%XMM6   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND %XMM8,%XMM13,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPBLENDVB %XMM15,%XMM12,%XMM2,%XMM10 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND %XMM6,%XMM13,%XMM9             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPACKUSDW %XMM5,%XMM3,%XMM4          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2999(%RIP),%XMM4,%XMM11      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPAND %XMM10,%XMM13,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPACKUSDW %XMM2,%XMM9,%XMM1          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2987(%RIP),%XMM1,%XMM0       | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSWB %XMM0,%XMM11,%XMM8         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU %XMM8,(%RBX,%RDX,1)          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nADD $0x10,%RDX                       | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nCMP $0xef0,%RDX                      | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nJNE 2510 <sobel_v4+0xa0>             | 1     | 0.50 | 0    | 0    | 0.50 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.50\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 6.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 32 bytes boundaries\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 32 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 32) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 43% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 33.50 to 14.00 cycles (2.39x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 33.50 to 28.00 cycles (1.20x speedup).\n",
        },
      },
      potential = {
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant non-unit stride: 1 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Avoid mixing data with different types. In particular, check if the type of constants is the same as array elements.",
          details = " - VPACKUSDW: 2 occurrences\n - VPACKUSWB: 1 occurrences\n",
          title = "Conversion instructions",
          txt = "Detected expensive conversion instructions.",
        },
        {
          title = "Type of elements and instruction set",
          txt = "",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is loading 256 bytes.\nThe binary loop is storing 96 bytes.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 140\nnb uops            : 139\nloop length        : 707\nused x86 registers : 11\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 0\nused zmm registers : 0\nnb stack references: 5\n",
        },
        {
          title = "Front-end",
          txt = "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 23.17 cycles\nfront end            : 23.17 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | ALU0 | ALU1 | ALU2 | ALU3 | AGU0  | AGU1  | FP0   | FP1   | FP2   | FP3\n----------------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.50 | 0.50 | 11.00 | 11.00 | 28.00 | 33.50 | 33.50 | 28.00\ncycles | 0.50 | 0.50 | 0.50 | 0.50 | 11.00 | 11.00 | 28.00 | 33.50 | 33.50 | 28.00\n\nCycles executing div or sqrt instructions: NA\nCycles loading/storing data              : 8.00\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 23.17\nDispatch  : 33.50\nData deps.: 1.00\nOverall L1: 33.50\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 100%\nload    : 100%\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 100%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 43%\nload    : 50%\nstore   : 50%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 50%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 39%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 33.50 cycles. At this rate:\n - 23% of peak load performance is reached (7.64 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 17% of peak store performance is reached (2.87 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 2510\n\nInstruction                          | Nb FU | ALU0 | ALU1 | ALU2 | ALU3 | AGU0 | AGU1 | FP0  | FP1  | FP2  | FP3  | Latency | Recip. throughput\n------------------------------------------------------------------------------------------------------------------------------------------------\nVMOVDQU (%RBP,%RDX,1),%XMM0          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVMOVDQU (%R13,%RDX,1),%XMM2          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM0,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM2,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM0,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM2,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM3,%XMM1,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%R11,%RDX,1),%XMM3          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM5,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM7,%XMM8                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM8,%XMM6,%XMM9             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM4,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM11,%XMM15              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM4,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVSXWD %XMM9,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM9,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R10,%RDX,1),%XMM9          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVSXWD %XMM2,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM15,-0x58(%RSP)           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM10,-0x68(%RSP)           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM0,-0x48(%RSP)            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVMOVDQA %XMM1,-0x38(%RSP)            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPSRLDQ $0x8,%XMM3,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM5,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM3,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM6,%XMM2                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM6,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVMOVDQU (%R8,%RDX,1),%XMM6           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXWD %XMM8,%XMM15               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R9,%RDX,1),%XMM8           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPMOVZXBW %XMM9,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM9,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM11,%XMM3               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM4,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM3,%XMM9                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM10,%XMM1            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM3,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM4,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXWD %XMM1,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM10,%XMM11              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXWD %XMM5,%XMM10               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA %XMM4,-0x78(%RSP)            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nVPMOVZXWD %XMM7,%XMM7                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM6,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM6,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM8,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM3,%XMM1,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVZXBW %XMM8,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM6,%XMM3                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU (%R14,%RDX,1),%XMM8          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM4,%XMM5              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSUBW %XMM3,%XMM1,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RDI,%RDX,1),%XMM1          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM4,%XMM3              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM8,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM1,%XMM6                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBW %XMM4,%XMM6,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQU (%RDI,%RDX,1),%XMM4          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSLLW $0x1,%XMM1,%XMM6              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM4,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVZXBW %XMM1,%XMM4                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPMOVZXBW %XMM8,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM0,%XMM11,%XMM8            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBW %XMM1,%XMM4,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM6,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM11,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM5,%XMM11               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM1,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x68(%RSP),%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM11,%XMM0,%XMM0            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM6,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSRLDQ $0x8,%XMM5,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPSLLW $0x1,%XMM4,%XMM4              | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM1,%XMM0,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x78(%RSP),%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPABSD %XMM8,%XMM8                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM11,%XMM0                  | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM0,%XMM8,%XMM11            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM6,%XMM0                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQA -0x58(%RSP),%XMM6            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPSUBD %XMM7,%XMM1,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM1,%XMM7,%XMM7             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM5,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSUBD %XMM2,%XMM9,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM0,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM9,%XMM2,%XMM9             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM3,%XMM2                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM6,%XMM8,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPABSD %XMM0,%XMM8                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM1,%XMM7,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM4,%XMM1                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPSRLDQ $0x8,%XMM4,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPADDD %XMM6,%XMM0,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM5,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM2,%XMM9,%XMM1             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPMOVSXWD %XMM4,%XMM9                | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPABSD %XMM6,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM7,%XMM8,%XMM8             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x48(%RSP),%XMM7            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPADDD %XMM7,%XMM0,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM7,%XMM1,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVMOVDQA -0x38(%RSP),%XMM1            | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 3       | 0.50\nVPABSD %XMM0,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM6,%XMM5                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPADDD %XMM7,%XMM5,%XMM6             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM15,%XMM10,%XMM5           | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSUBD %XMM10,%XMM15,%XMM15          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPSRLDQ $0x8,%XMM3,%XMM10            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 1       | 1\nVPMOVSXWD %XMM10,%XMM3               | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPADDD %XMM9,%XMM5,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM3,%XMM15,%XMM5            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM2,%XMM0             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPADDD %XMM1,%XMM5,%XMM4             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD %XMM14,%XMM11,%XMM1         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM0,%XMM7                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPABSD %XMM4,%XMM9                   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPCMPGTD %XMM14,%XMM8,%XMM0          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM1,%XMM12,%XMM11,%XMM11 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPADDD %XMM9,%XMM7,%XMM2             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.33 | 0.33 | 0    | 0.33 | 1       | 0.33\nVPCMPGTD %XMM14,%XMM6,%XMM7          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM0,%XMM12,%XMM8,%XMM8   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND %XMM11,%XMM13,%XMM3            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPCMPGTD %XMM14,%XMM2,%XMM15         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0    | 0    | 0.50 | 1       | 0.50\nVPBLENDVB %XMM7,%XMM12,%XMM6,%XMM6   | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND %XMM8,%XMM13,%XMM5             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPBLENDVB %XMM15,%XMM12,%XMM2,%XMM10 | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    | 1       | 1\nVPAND %XMM6,%XMM13,%XMM9             | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPACKUSDW %XMM5,%XMM3,%XMM4          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2999(%RIP),%XMM4,%XMM11      | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPAND %XMM10,%XMM13,%XMM2            | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.25\nVPACKUSDW %XMM2,%XMM9,%XMM1          | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVPAND 0x2987(%RIP),%XMM1,%XMM0       | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0.25 | 0.25 | 0.25 | 0.25 | 1       | 0.50\nVPACKUSWB %XMM0,%XMM11,%XMM8         | 1     | 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 1       | 0.50\nVMOVDQU %XMM8,(%RBX,%RDX,1)          | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1    | 0    | 4       | 1\nADD $0x10,%RDX                       | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nCMP $0xef0,%RDX                      | 1     | 0.25 | 0.25 | 0.25 | 0.25 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.25\nJNE 2510 <sobel_v4+0xa0>             | 1     | 0.50 | 0    | 0    | 0.50 | 0    | 0    | 0    | 0    | 0    | 0    | 1       | 0.50\n",
        },
      },
      header = {
        "0% of peak computational performance is used (0.00 out of 6.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = "Use vector aligned instructions:\n 1) align your arrays on 32 bytes boundaries\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 32 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 32) and use it instead of 'foo' in the loop.\n",
          details = "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is vectorized, but using 43% register length (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 33.50 to 14.00 cycles (2.39x speedup).",
        },
        {
          workaround = "Reduce arithmetical operations on array elements",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of INT/FP operations in vector registers (the VPU is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 33.50 to 28.00 cycles (1.20x speedup).\n",
        },
      },
      potential = {
      },
    },
  common = {
    header = {
      "The loop is defined in /home/vidal/Desktop/AOC_oseret/Projet/AOC-Computer-vision/sobel.c:240-251.\n",
      "It is main loop of related source loop which is unrolled by 16 (including vectorization).",
    },
    nb_paths = 1,
  },
}
